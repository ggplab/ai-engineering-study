# 03단원

    
    ### 💡 문제 출제
    
    - **문제 1:** 언어 모델의 '혼잡도(Perplexity)'는 '교차 엔트로피(Cross Entropy)'와 어떤 관계이며, 이 지표가 근본적으로 무엇을 측정하는지 설명하세요.
        
        <aside>
        💡
        
        혼잡도(Perplexity, PPL)는 엔트로피와 교차 엔트로피(Cross Entropy, CE)에의 지수함수
        
        두 지표 모두 언어 모델이 다음에 올 단어(또는 토큰)를 얼마나 잘 예측하는지를 측정
        
        - 혼잡도(Perplexity) : 다음 토큰을 예측할때의 불확실성을 측정
        - 교차 엔트로피(Cross Entropy) : 모델이 다음 토큰을 예측하기 얼마나 어려운지 측정
        - 교차 엔트로피 & 혼잡도 간단 예시 정리
        
               문장: "오늘은 날씨가 매우 _ _ _."
        
               정답 단어: "좋다"
        
               모델의 예측 확률 예시: 좋다: 0.60 / 춥다: 0.20 / 덥다: 0.10 / 흐리다: 0.10
        
               1) 교차 엔트로피(Cross Entropy)
        
                정답 “좋다”에 0.60(60%) 을 줬으므로 모델이 정답을 꽤 자신 있게 예측한 상태
        
                → 교차 엔트로피는 낮음 (정답 확률이 높을수록 CE는 낮아짐)
        
               2) 혼잡도(Perplexity, PPL)
        
                교차 엔트로피를 바탕으로 계산됨
        
                정답 확률이 0.60으로 높기 때문에 → 혼잡도도 낮음 (헷갈리지 않았다는 뜻)
        
        **결론: 정답 단어에 높은 확률을 줄수록 → 교차 엔트로피↓, 혼잡도↓ → 모델이 잘 예측한 것!**
        
        </aside>
        
    - **문제 2:** '정확한 평가(Exact Evaluation)' 방법론 중 '기능적 정확성(Functional Correctness)'은 무엇이며, 이 방식이 코드 생성이나 Text-to-SQL 같은 태스크에 특히 유용한 이유를 설명하세요.
        
        <aside>
        💡
        
        기능적 정확성 평가 : 시스템이 의도한 기능을 제대로 수행하는지 평가하는 것
        
        예시) 입력을 넣었을 때 기대한 출력이 정확히 나오는가? → 이게 바로 기능적 정확성
        
        - 코드 생성에 유용한 이유 : 코드 스타일, 구조, 길이는 달라도 실행 결과가 맞으면 정답
        
        ```python
        # for문으로 만든 코드
        def sum_even(nums):
            total = 0
            for n in nums:
                if n % 2 == 0:
                    total += n
            return total
            
        # 리스트 컴프리헨션으로 만든 코드
        def sum_even(nums):
            return sum([n for n in nums if n % 2 == 0])
        #-------------------------------------------------------
        # 입력:
        sum_even([1,2,3,4,5,6])
        # 출력:
        12
        ```
        
        - Text-to-SQL에 유용한 이유 : 문자열 그대로 비교하면 전부 다른 쿼리지만 실행하면 똑같은 결과 → 기능적으로 동일 → 정답
        
        문제: 40살 이상인 사람의 이름을 조회하라.
        
        테이블: `users(name, age)`
        
        ```sql
        SELECT name FROM users WHERE age >= 40;    # 조건 왼쪽 기준
        SELECT name FROM users WHERE 40 <= age;    # 조건을 뒤집어쓴 버전
        SELECT name FROM users WHERE (age >= 40);  # 불필요한 괄호 포함
        ```
        
        **결론: 코드도 SQL도 겉모양은 달라질 수 있지만 실행 결과가 맞으면 정답이므로 기능적 정확성이 가장 정확한 평가 방식**
        
        </aside>
        
    - **문제 3:** '어휘적 유사성(Lexical Similarity)'과 '의미론적 유사성(Semantic Similarity)'은 텍스트를 비교하는 방식에서 어떤 근본적인 차이가 있는지 설명하세요.
        
        <aside>
        💡
        
        **어휘적 유사성(Lexical Similarity):**  두 텍스트가 겉으로 얼마나 비슷하게 생겼는지(형태적 유사성)를 측정. 이를 위해 텍스트를 더 작은 토큰(단어·문자 등)으로 나누어 그 겹치는 정도를 비교. 이 방식은 의미는 고려하지 않고, 문자나 단어가 얼마나 일치하는지만 판단함.
        → 예: “고양이”와 “고양잇과”는 철자가 비슷해 유사도가 높지만, “고양이”와 “냥이”는 의미는 같아도 철자가 다르므로 유사도가 낮게 나옴.
        
        **의미론적 유사성(Semantic Similarity):** 두 텍스트가 의미적으로 얼마나 가까운지를 평가. 단어·문장을 벡터(임베딩) 형태로 숫자로 변환한 후, 그 거리나 방향을 비교해 의미적 유사성을 계산. 즉, 형태가 다르더라도 뜻이 같으면 높은 유사도를 갖음.
        → 예: “학생이 시험을 봤다”와 “시험이 학생에 의해 치러졌다”는 표현은 다르지만 의미가 같아 높은 유사도를 가진다. 마찬가지로 “고양이”와 “냥이”도 의미적으로 유사도가 높게 측정됨.
        
        </aside>
        
    - **문제 4:** '임베딩(Embedding)'이란 무엇이며, CLIP과 같은 다중 모드 임베딩 모델이 텍스트와 이미지를 어떻게 '공동 임베딩 공간(joint embedding space)'으로 매핑하는지 그 목적을 설명하세요.
        
        <aside>
        💡
        
        - 임베딩 : 컴퓨터가 처리할 수 있도록 데이터를 수치(벡터)로 표현하여 커.
            
             좋은 임베딩 알고리즘은 더 유사한 텍스트들이 더 가까운 임베딩을 가지도록 벡터를 생성하는 것
            
        - CLIP과 같은 다중 모드 임베딩 모델의 작동 방식
        :서로 다른 데이터 양식(이미지, 텍스트)을 하나의 공동 임베딩 공간에 매핑함으로써, 이질적인 데이터 간의 의미적 유사성을 계산하고 교차 검색을 가능하게 하여 더 강력한 AI 애플리케이션의 기반을 마련.
            
            마치 다른 언어로 쓰인 두 권의 책을 같은 도서관의 같은 주제 코너에 배치하여, 내용의 유사성을 언어가 아닌 주제로 찾을 수 있게 하는 것과 같음
            
            - 공동 임베딩 공간(Joint Embedding Space)
                
                서로 다른 데이터 양식 간의 비교 및 결합을 가능하게 하는 다중 모드 임베딩 공간
                
        </aside>
        
    - **문제 5:** 'AI 심사위원(AI as a Judge)' 접근 방식이 기존의 인간 평가자에 비해 가지는 주요 이점 2가지를 설명하세요.
        
        <aside>
        💡
        
        1. 속도, 비용 효율성 및 유연성 (Fast, Cheap, and Flexible)
        
        - AI 심사위원은 인간 평가에 비해 훨씬 빠르고 사용하기 쉬우며 상대적으로 저렴
        - AI 모델에게 정확성, 반복성, 유해성, 환각(hallucinations) 등 어떤 기준으로든 출력을 평가하도록 요청할 수 있음→ 사람이 다양한 의견을 제시할 수 있는 것과 유사
        - 결정 설명 능력 : 자신의 판단에 대한 이유를 설명할 수 있음
        1. 참조 데이터 불필요 및 프로덕션 적용 용이성
        - 정답표(Reference Data)가 없어도 평가할 수 있음
            
            (기존 정답표: 모델이 낸 답 ⟶ 정답표와 비교 ⟶ 맞으면 1점 / 틀리면 0점)
            
            ⇒ 실제 서비스에서 나온 응답도 바로 평가하고 모델 개선에 활용 가능
            
        
        결론 :
        AI 심사위원은 사람보다 더 일관되고 흔들림 없는 평가를 제공하며, 정답이 없어도 기준 기반으로 평가할 수 있어 실제 서비스에서도 활용하기 쉽다
        
        </aside>
        
    - **문제 6:** 코드 생성 벤치마크(예: HumanEval)에서 사용하는 `pass@k` 메트릭은 모델의 성능을 어떻게 측정하는지 (k=3을 예시로) 설명하세요.
    
    <aside>
    💡
    
    </aside>
    
    - **문제 7:** '비교 평가(Comparative Evaluation)'와 'A/B 테스팅'은 사용자가 응답을 경험하고 피드백을 제공하는 방식에서 어떻게 다른지 설명하세요.
    
    <aside>
    💡
    
    비교평가: 
    - 동시노출 : 사용자가 두개의 응답을 동시에 경험함
    a/b테스팅 : 
    - 분리된 노출 : 사용자가 한번에 하나의 응답만을 경험함.
    
    </aside>
    
    - **문제 8:** '혼잡도(Perplexity)'는 모델이 텍스트를 얼마나 잘 예측하는지 측정합니다. 이 원리를 이용해 **(1) 데이터 오염(Data Contamination)**과 **(2) 비정상적 텍스트**를 탐지하는 방법을 각각 설명하세요.
    
    <aside>
    💡
    
    </aside>
    
    - **문제 9:** AI 심사위원 사용 시 발생할 수 있는 편향 중, '자기 편향(Self-bias)'과 '첫 위치 편향(Position-bias)'이 각각 무엇인지 설명하세요.
    
    <aside>
    💡
    
    자기 편향 - AI 평가자가 자기가 생성한 응답을 다른 모델이 생성한 응답보다 더 높게 평가하는 편향. 
    
    첫 위치  편향 - 평가 대상이 짝을 이루고 있거나 여러 선택지가 있을 때 AI 평가자가 첫번째로 나오는 대상 또는 선택지를 더 높게 평가하는 편향
    
    </aside>
    
    - **문제 10:** 응답의 주관적인 품질을 평가할 때, '점수 부여(pointwise) 평가' 방식보다 '쌍대 비교(pairwise comparison)' 방식이 더 선호되는 이유를 설명하세요.
    
    <aside>
    💡
    
    주관적 품질을 평가할 때 점수 부여 방식은 각 모델의 응답을 독립적으로 평가한 뒤 점수를 가지고 비교대조한다. 그러나 이러한 방법으로 각 모델의 점수를 구체화하여 비교하는 것보다 두 모델의 응답에 대해 상대적 비교를 통한 선호도 평가가 더 쉽다고 한다. 그러나 모든 질의에 대해 이렇게 선호도를 바탕으로 평가할 수는 없다. 때로는 구체적인 기준에 따라 정확성을 평가해야 하는 과제도 있다.
    
    </aside>
    
    - **문제 11:** 'AI 심사위원'의 '장황함 편향(Verbosity-bias)'이란 무엇이며, 이 편향이 어떻게 부정확한 응답을 긍정적으로 평가하는 왜곡을 발생시킬 수 있는지 설명하세요.
    
    <aside>
    💡
    
    AI 평가자가 응답의 품질과 관계 없이 더 긴 응답에 대해 높이 평가하는 것.
    
    → 간결하고 정확한 답보다 장황하고 틀린 답을 옳다고 잘못 평가할 수 있다.
    
    </aside>
    
    - **문제 12:** AI 심사위원이 평가 대상 모델보다 '약한 모델'이어도 되는 근거는 무엇이며, RLHF에 사용되는 '보상 모델(Reward Model)'이 어떻게 이 원리를 활용하는지 설명하세요.
    
    <aside>
    💡
    
    일반적으로 LLM은 텍스트를 생성하는 것보다 주어진 텍스트를 분류/평가하는 과제를 더 쉽게 수행한다. 이를 근거로  약한 모델을 AI 평가자로 사용할 수 있다고 한다. (한편, flagship 모델로 평가하면 비용이 많이 발생한다.)
    
    특화된 평가자에서 보상 모델은 (프롬프트, 응답) 쌍이 주어지면 응답의 정확성에 대해 0 ~ 1 값으로 평가한다. 보상 모델은 이러한 방식의 평가 과제에 대해 특화된 소형 모델이다. 특화된 평가자 모델은 특정 평가 과제에 대해 대형 범용모델보다 더 신뢰할 수 있는 예측을 할 수 있다고 한다.
    
    </aside>
    
    - **문제 13:** 파운데이션 모델 평가는 기존 ML 모델 평가보다 더 어렵습니다. 그 주된 이유 두 가지를 **(1) 모델의 응답 특성(예: 개방형)**과 **(2) 모델의 지능 수준** 관점에서 설명하세요.
    
    <aside>
    💡
    
    1. 파운데이션 모델은 학습한 데이터 분포를 바탕으로 주어진 입력 토큰 다음에 나타날 토큰을 예측한다. 그러나 언어 모델의 특성 상 예측 토큰 정확도 뿐만 아니라 생성된 텍스트의 문맥적 의미를 평가할 수 있어야 한다(정답이 여러 개일 수 있음). 이러한 점에서 개방형 모델은 그 응답을 평가할 때 주관적 고려 요소가 있으므로 기존 ML 모델을 평가하는 것보다 훨씬 더 까다롭다.
    2. 모델의 지능 수준 관점에서는 만약 최신 Gemini-3 모델이 수학 논문에 대해 정리한 내용을 평가한다고 할 때 모델의 출력에 대해 평가할 수 있는 사람은 많지 않을 것이다. 왜냐하면 일반적인 사람들은 대학원 수준의 수학에 대해 잘 아는 사람이 없기 때문이다. 이렇게 파운데이션 모델이 고도의 추론과 전문지식에 관한 내용을 출력할 때 이를 평가하기 위해서는 비슷한 수준 또는 그 이상의 지식과 자료가 필요할 수 있다. 이러한 검증을 할 수 있는 사람 많지 않다. 모델이 똑똑하고 전문적일 수록 이를 평가하기는 쉽지 않다.
    </aside>
    
    - **문제 14:** '비교 평가' 시스템에서 'Elo'나 'Bradley-Terry'와 같은 평가 알고리즘(Rating Algorithm)은 수집된 쌍대 비교 결과를 어떻게 처리하여 모델의 최종 순위를 결정하는지 설명하세요.
    
    <aside>
    💡
    
    한 모델이 다른 하나의 모델과 match에서 이긴 횟수를 확률로 바꿔 점수로 사용한다. 이러한 점수들을 종합하여 모든 모델들의 점수를 순위로 매겨 최종 순위를 결정한다.
    
    </aside>
    
    - **문제 15:** 크라우드소싱을 통한 '비교 평가'(예: 챗봇 아레나)는 **(1) 확장성** 측면과 **(2) 표준화 및 품질 관리** 측면에서 어떤 한계점을 가지는지 설명하세요.
    
    <aside>
    💡
    
    1. 비교 평가에서 새로운 모델이 추가되는 경우 기존 모델의 순위를 바꿀 수 있다. 그리고 비공개 모델을 평가해야 하는 경우 비교 데이터를 직접 수집해서 비교를 하거나 비교 업체에 비공개 평가를 의뢰하는 식으로 진행해야 한다.
    2. 불특정 다수의 다양한 사람들이 익명의 모델에 대해 평가하기 때문에 표준화 및 품질 문제가 발생한다. 전문적으로 어떤 응답이 적절한지 평가할 능력이 없는 사용자가 선호도를 평가하는 경우, 악의적이고 유해한 응답을 선호하는 사용자가 평가하는 경우 등 모델 성능 평가를 일관되게 유지하기 어려운 문제가 있다.
    </aside>
    
    - **문제 16:** '비교 평가'는 모델의 상대적 순위를 알려주지만, "이 모델이 우리 용도에 충분히 좋은가?"라는 질문에는 답하기 어렵습니다. 그 이유를 '절대적 성능' 관점에서 설명하세요.
    
    <aside>
    💡
    
    두 모델 중 어떤 모델이 더 좋은 모델인지는 알 수 있지만 절대적인 성능을 파악하기는 어렵다. (예: 두 모델 다 성능이 떨어질 경우)
    
    </aside>
    
    ---
    
    ### 📝 예시 답안
    
    - **문제 1:** 언어 모델의 '혼잡도(Perplexity)'는 '교차 엔트로피(Cross Entropy)'와 어떤 관계이며, 이 지표가 근본적으로 무엇을 측정하는지 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        '혼잡도(Perplexity, PPL)'는 **'교차 엔트로피(Cross Entropy)'의 지수 값**입니다. 이 지표는 모델이 다음 토큰을 예측할 때 갖는 '불확실성(uncertainty)'의 양을 측정합니다. 즉, 모델이 평균적으로 몇 개의 가능한 토큰 중에서 고민하는지를 나타내며, 값이 낮을수록 모델이 텍스트를 더 정확하게 예측한다는 의미입니다.
        
        </aside>
        
    - **문제 2:** '정확한 평가(Exact Evaluation)' 방법론 중 '기능적 정확성(Functional Correctness)'은 무엇이며, 이 방식이 코드 생성이나 Text-to-SQL 같은 태스크에 특히 유용한 이유를 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        '기능적 정확성'은 모델의 생성물이 의도한 기능을 올바르게 수행했는지 여부를 평가하는 것입니다. 이 방식은 생성된 코드나 SQL 쿼리를 **실제로 실행**하여 유닛 테스트를 통과하는지, 또는 쿼리 결과가 예상된 값과 일치하는지 등을 자동으로 검증할 수 있기 때문에 해당 태스크들에 특히 유용합니다 .
        
        </aside>
        
    - **문제 3:** '어휘적 유사성(Lexical Similarity)'과 '의미론적 유사성(Semantic Similarity)'은 텍스트를 비교하는 방식에서 어떤 근본적인 차이가 있는지 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        '어휘적 유사성'은 두 텍스트가 **표면적으로 얼마나 비슷하게 보이는지** (예: 겹치는 단어, n-그램 수)를 측정합니다 . 반면, '의미론적 유사성'은 두 텍스트의 **'의미(semantics)'가 얼마나 가까운지**를 측정합니다. 예를 들어 "What's up?"과 "How are you?"는 어휘적으로는 매우 다르지만 의미론적으로는 매우 가깝습니다.
        
        </aside>
        
    - **문제 4:** '임베딩(Embedding)'이란 무엇이며, CLIP과 같은 다중 모드 임베딩 모델이 텍스트와 이미지를 어떻게 '공동 임베딩 공간(joint embedding space)'으로 매핑하는지 그 목적을 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        '임베딩'은 텍스트와 같은 원본 데이터의 '의미(meaning)'를 포착하도록 설계된 숫자 벡터 표현입니다. CLIP과 같은 '다중 모드 임베딩' 모델의 목적은 텍스트와 이미지처럼 서로 다른 유형의 데이터를 하나의 '공동 임베딩 공간'으로 매핑하는 것입니다. 이를 통해 "낚시하는 남자"라는 텍스트의 임베딩이 실제 낚시하는 남자의 이미지 임베딩과 벡터 공간상에서 가까워지도록 하여, 텍스트 기반 이미지 검색 등을 가능하게 합니다 .
        
        </aside>
        
    - **문제 5:** 'AI 심사위원(AI as a Judge)' 접근 방식이 기존의 인간 평가자에 비해 가지는 주요 이점 2가지를 설명하세요.
        
        <aside>
        💡
        
        **답안:**
        
        - **속도와 비용:** AI 심사위원은 인간 평가자보다 훨씬 빠르고 저렴하게 대규모의 응답을 평가할 수 있습니다.
        - **참조 데이터 불필요:** 참조(정답) 데이터 없이도 모델의 응답 품질을 평가할 수 있어, 참조 데이터를 만들기 어려운 실제 프로덕션 환경에서도 유용합니다. (또한, AI 심사위원은 평가에 대한 '설명'이나 '이유'를 함께 제공할 수 있습니다.)
        </aside>
        
    - **문제 6:** 코드 생성 벤치마크(예: HumanEval)에서 사용하는 `pass@k` 메트릭은 모델의 성능을 어떻게 측정하는지 (k=3을 예시로) 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        `pass@k`는 모델이 특정 문제에 대해 `k`개의 코드 샘플을 생성하도록 했을 때, 이 `k`개의 샘플 중 하나라도 모든 유닛 테스트를 통과하면 해당 문제를 '해결'한 것으로 간주하는 평가 메트릭입니다. 예를 들어 `pass@3`은, 모델이 3개의 코드 샘플을 생성하고 그 3개 중 단 하나라도 유닛 테스트를 통과하면 성공으로 봅니다. 최종 점수는 전체 문제 중 성공적으로 해결한 문제의 비율로 계산됩니다.
        
        </aside>
        
    - **문제 7:** '비교 평가(Comparative Evaluation)'와 'A/B 테스팅'은 사용자가 응답을 경험하고 피드백을 제공하는 방식에서 어떻게 다른지 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        A/B 테스팅은 사용자가 한 번에 하나의 모델 후보(A 또는 B)만 경험하고 그에 대한 암시적 피드백(예: 클릭률)을 수집하는 방식입니다. 반면, 비교 평가는 사용자(또는 평가자)에게 두 개 이상의 모델 응답을 동시에 제시하고 어느 것이 더 나은지 직접 선택하도록(명시적 피드백) 하는 방식입니다.
        
        </aside>
        
    - **문제 8:** '혼잡도(Perplexity)'는 모델이 텍스트를 얼마나 잘 예측하는지 측정합니다. 이 원리를 이용해 **(1) 데이터 오염(Data Contamination)**과 **(2) 비정상적 텍스트**를 탐지하는 방법을 각각 설명하세요.
        
        <aside>
        💡
        
        **답안:**
        
        - **데이터 오염 탐지:** 모델은 훈련 중에 이미 보았던(암기한) 텍스트에 대해 매우 낮은 혼잡도(PPL)를 보입니다. 만약 모델이 특정 벤치마크 데이터에 대해 비정상적으로 낮은 혼잡도를 보인다면, 이는 해당 벤치마크가 모델의 훈련 데이터에 포함되었을 가능성(오염)을 시사합니다 .
        - **비정상적 텍스트 탐지:** 혼잡도는 예측 불가능한 텍스트일수록 높게 나타납니다. 따라서 횡설수설하는 텍스트나 비정상적인 아이디어(예: "우리 집 개는 양자물리학을 가르친다")를 포함한 텍스트는 높은 혼잡도 값을 가지므로, 이를 탐지하는 데 혼잡도를 사용할 수 있습니다.
        </aside>
        
    - **문제 9:** AI 심사위원 사용 시 발생할 수 있는 편향 중, '자기 편향(Self-bias)'과 '위치 편향(Position-bias)'이 각각 무엇인지 설명하세요.
        
        <aside>
        💡
        
        **답안:**
        
        - **자기 편향 (Self-bias):** AI 심사위원 모델이 다른 모델이 생성한 응답보다 자기 자신이 생성한 응답을 더 선호하는(높은 점수를 주는) 경향입니다.
        - **위치 편향 (Position-bias):** 두 개의 응답을 비교 평가할 때, 응답의 품질과 상관없이 첫 번째 위치에 제시된 응답을 선호하는 경향입니다. 이는 인간의 '최신 편향(recency bias)'과는 반대되는 경향입니다.
        </aside>
        
    - **문제 10:** 응답의 주관적인 품질을 평가할 때, '점수 부여(pointwise) 평가' 방식보다 '쌍대 비교(pairwise comparison)' 방식이 더 선호되는 이유를 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        주관적인 품질에 대해 '절대적인 점수'(예: 1~5점)를 일관되게 매기는 것은 인간(또는 AI) 평가자에게 매우 어려운 작업입니다. 한 평가자가 5점을 준 응답과 다른 평가자가 7점을 준 응답을 비교하기 어렵습니다. 반면, 두 개의 응답을 나란히 놓고 '어느 것이 더 나은지'를 판단하는 '상대적인 비교'는 훨씬 더 쉽고 일관되게 수행할 수 있기 때문에 쌍대 비교 방식이 더 선호됩니다.
        
        </aside>
        
    - **문제 11:** 'AI 심사위원'의 '장황함 편향(Verbosity-bias)'이란 무엇이며, 이 편향이 어떻게 부정확한 응답을 긍정적으로 평가하는 왜곡을 발생시킬 수 있는지 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        '장황함 편향'은 AI 심사위원이 응답의 실제 품질이나 정확성과 관계없이 더 길고 장황한(verbose) 응답을 더 좋다고 평가하는 경향입니다. 이 편향은 평가 결과를 왜곡시킬 수 있는데, 한 연구에서는 AI 심사위원이 사실적 오류가 포함된 긴 응답을, 오류 없이 정확하지만 짧은 응답보다 선호하는 현상을 발견했습니다. 이는 길이와 품질을 혼동하여 부정확한 응답에 더 높은 점수를 주게 만듭니다.
        
        </aside>
        
    - **문제 12:** AI 심사위원이 평가 대상 모델보다 '약한 모델'이어도 되는 근거는 무엇이며, '보상 모델(Reward Model)'이 어떻게 이 원리를 활용하는지 설명하세요.
        
        <aside>
        💡
        
        **답안:**
        
        - **근거:** 응답을 '생성'하는 것(예: 노래 작곡)이 응답을 '판단'하는 것(예: 노래가 좋은지 평가)보다 더 어려운 작업으로 간주되기 때문입니다. 따라서 더 약한 모델도 더 강한 모델의 출력을 판단할 수 있습니다.
        - **활용:** '보상 모델(Reward Model)'은 (프롬프트, 응답) 쌍을 입력받아 응답이 얼마나 좋은지 점수를 출력하도록 전문화된 모델입니다. 이러한 모델(예: Cappy)은 종종 평가 대상인 거대 모델보다 크기가 훨씬 작지만(예: 360M 파라미터), 특정 기준(예: 정확성)에 대해 효과적인 평가를 수행할 수 있습니다.
        </aside>
        
    - **문제 13:** 파운데이션 모델 평가는 기존 ML 모델 평가보다 더 어렵습니다. 그 주된 이유 두 가지를 **(1) 모델의 응답 특성(예: 개방형)**과 **(2) 모델의 지능 수준** 관점에서 설명하세요.
        
        <aside>
        💡
        
        **답안:**
        
        - **모델의 응답 특성:** 파운데이션 모델은 정답이 하나로 정해지지 않은 '개방형(Open-ended)' 응답을 생성합니다. 기존 ML 모델(예: 분류)은 출력이 미리 정의된 범주로 한정되어 정답과 비교하기 쉽지만, 파운데이션 모델은 다양한 정답 표현이 가능해 참조 데이터와 비교하기 어렵습니다.
        - **모델의 지능 수준:** 모델이 지능화될수록 평가가 더 어려워집니다. 1학년 수학 문제와 박사 수준의 수학 문제를 평가하는 난이도가 다르듯이, 모델이 생성하는 정교한 요약이나 분석은 평가자(인간 또는 AI)가 그 내용을 완전히 이해하고 사실 확인까지 해야 하므로 훨씬 더 시간 소모적입니다 .
        </aside>
        
    - **문제 14:** '비교 평가' 시스템에서 'Elo'나 'Bradley-Terry'와 같은 평가 알고리즘(Rating Algorithm)은 수집된 쌍대 비교 결과를 어떻게 처리하여 모델의 최종 순위를 결정하는지 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        이 알고리즘들은 수집된 수많은 '쌍대 비교 결과'(예: 모델 A가 모델 B를 이김, 모델 C가 모델 A를 이김 등)를 통계적으로 취합하여, 각 모델의 상대적인 강점을 나타내는 '단일 점수(Score)'를 계산하는 역할을 합니다. 이 계산된 점수를 기준으로 전체 모델의 순위를 매길 수 있습니다.
        
        </aside>
        
    - **문제 15:** 크라우드소싱을 통한 '비교 평가'(예: 챗봇 아레나)는 **(1) 확장성** 측면과 **(2) 표준화 및 품질 관리** 측면에서 어떤 한계점을 가지는지 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        - **확장성 병목 현상:** 평가해야 할 모델의 수가 증가함에 따라 필요한 비교 쌍의 수가 기하급수적으로(quadratically) 증가합니다. 57개의 모델을 비교하는 데 244,000건의 비교가 사용되었음에도 쌍당 평균 153건에 불과할 정도로, 모든 모델을 충분히 비교하기 어렵습니다.
        - **표준화 및 품질 관리의 부재:** 평가자들이 사용하는 프롬프트가 표준화되어 있지 않고, 무엇이 '더 나은' 응답인지에 대한 기준도 제각각입니다. 이로 인해 사실 확인 없이 그럴듯하게 들리는 응답을 선호하거나, "안녕"과 같은 너무 단순한 프롬프트를 사용하여 모델 간의 변별력을 떨어뜨리는 등 노이즈가 많은 피드백이 수집될 수 있습니다.
        </aside>
        
    - **문제 16:** '비교 평가'는 모델의 상대적 순위를 알려주지만, "이 모델이 우리 용도에 충분히 좋은가?"라는 질문에는 답하기 어렵습니다. 그 이유를 '절대적 성능' 관점에서 설명하세요.
        
        <aside>
        💡
        
        **답안:** 
        
        비교 평가는 모델 간의 '상대적인 순위'는 알려주지만, 각 모델의 '절대적인 성능'이나 '업무 적합성(good enough)'을 알려주지 않기 때문입니다 . 예를 들어, 모델 B가 모델 A보다 낫다고 해도, 
        
        (1) 두 모델 모두 사용 불가능할 정도로 나쁘거나, 
        
        (2) 두 모델 모두 이미 충분히 좋을 수 있습니다. 또한 "51%의 선호도"가 실제 비즈니스 메트릭(예: 고객 문제 해결률)에 얼마나 큰 향상을 가져올지 알 수 없으므로, 추가 비용을 정당화하기 어렵습니다.
        
        </aside>
        
    
