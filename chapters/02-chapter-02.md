# 02단원

    
    ### 💡 스터디 질문
    
    - 1 모델 규모(Model Size)를 나타내는 세 가지 핵심 지표(파라미터 수, 학습 토큰 수, FLOPs)와 각각이 모델의 어떤 측면을 대변하는지 설명하세요.
        
        <aside>
        💡
        
        1. 파라미터 수는 진행중인 모델을 학습하고 실행하는데 필요한 컴퓨터 자원(용량)을 추정할 수 있음.
        
        파라미터 수가 0인 값이 높다는 의미는 0은 곱해도 0이기 때문에 모델이 그부분을 계산하지 않아도 되서 사용하지 않는 파라미터가 많지 않게됨. 모델이 전체 구조 대비 실제로 활용되는 부분이 적은 ‘희소한 모델’이라고 함. 전체 연산량이 줄어들어 모델이 커도 그보다 더 적은 자원으로 모델을 돌려볼 수 있음.
        
        대표 모델로는 MoE가 있음(이 모델은 수업때 들은 기억은 있는데 진행한 프로젝트에서는 실행해보지 않아서 한 번 해봐야 체감할 수 있을 것 같음)
        
        2. 학습 토큰 수는 모델이 실제로 학습한  토큰의 총량(실제로 읽고 학습한 양)을 의미함.
        
        토큰=단어를 쪼갠 최소 단위(LLM에서 최소 단위)
        
        토큰을 많이 읽을수록 계산해야할 양도 같이 늘어나고..컴퓨팅 자원이 많이 드니까.. 앞으로 자주 실습해보려면 돈 마니 벌어서 GPU 사야될 것 같아요😭
        
        3. FLOPs (Floating Point Operations, 부동소수점 연산 횟수)는 얼마나 많은 연산을 했는지 나타내는 전체 계산량을 의미함.
        
        모델을 만들기 위해 얼마나 큰 계산(연산)량을 돌렸는지를 보여주는 지표
        
        * 참고사항: FLOP/s랑 FLOPs로 표기되는 사례가 있는데
        
        - FLOPs : 부동소수점 연산 횟수
        
        - FLOP/s (/s=per second): 초당 몇개의 FLOPs(부동소수점 연산 횟수)을 처리하는지 나타내는 성능 단위
        
        </aside>
        
    - 2 모델 개발 예산이 고정되어 있을 때, 컴퓨팅 최적 모델(Compute-Optimal Model)을 구축하기 위한 스케일링 법칙(Scaling Law)의 핵심 원칙을 설명하세요.
        
        <aside>
        💡
        
        고정된 예산 내에서 최고 성능을 내기 위해서 모델의 파라미터를 키우는 것과 학습 토큰 수를 늘리는 것을 균형 있게 병행하는 것
        
        * 참고사항
        
        1) 친칠라 스케일링 법칙(Chinchilla Scaling Law) : 
        
        - 2022년 DeepMind 발표, **파라미터 수 대비 약 20배**의 학습 토큰이 최적
        
              즉, 모델 크기를 2배로 늘리면 훈련 데이터양도 **2배로 늘려야** 가장 효율적(1:1 스케일링 비율)
        
        - 2023년 이후, 훈련 목적, 모델 구조, 사용 환경(추론 비용)에 따라 최적 비율이 달라질 수 있다고 규칙 해석 확장
        
               훈련만 고려 시 1:20 비율이 훈련 예산 대비 효율적
        
               추론까지 고려 시: 훈련은 한 번만 하는데 추론(실제 사용)은 수십억 번 반복되기 때문에 훈련 예산을 조금 더 사용하더라도 모델 크기를 더 작게 만들고, 대신 더 많은 데이터로 더 오래 학습시키는 것이 총비용 면에서 가장 효율적인 전략이라는 결론 제시
        
        </aside>
        
    - 3 사전 훈련된 모델(Pre-trained Model)이 일반적으로 가지는 두 가지 주요 문제점을 설명하고, 사후 훈련(Post-Training)이 이 문제들을 어떻게 해결하는지 설명하세요.
        
        <aside>
        💡
        
        **사전 학습 모델 문제점:** 
        
        1) 자기 지도 학습은 대화가 아닌 단어(토큰) 완성을 잘하도록 학습
        
        - 질문에 답하지 않고 문장을 그냥 이어 쓰는 문제가 나타남
        - 사용자 의도와 상관없이 반말·존댓말 등 어투가 제멋대로 생성됨
        - ‘요약해줘·정리해줘’ 같은 지시를 수행하지 못하고 원문을 그대로 늘어놓음
        - 대화 맥락을 이해하지 못해 앞뒤 흐름이 끊기는 응답이 나옴
        
        2) 인터넷에서 수집한 데이터로 학습하여 잘못된 정보 반영
        
        - 인종·성별 고정관념이 무의식적으로 드러나는 편향 문제
        - 존재하지 않는 정보나 사실을 만들어내는 할루시네이션 발생
        - 웹 데이터 특성상 공격적 표현이나 부적절한 내용이 섞일 위험
        
        **사후 학습 모델 해결방안:**
        
        1) 지도 파인튜닝(Supervised Fine-Tuning, SFT): 자기지도학습의 한계를 해결하는 단계
        
        - 사람이 직접 만든 “좋은 답변 예시”로 모델을 다시 학습시킨 후, 질문에 정확한 대답/ 지시 수행/ 대화 흐름 유지하는 능력 강화
        - 즉, “텍스트 이어쓰기 모델”을  “대화 가능한 모델”로 바꿔주는 과정
        
        2) 선호도 파인튜닝(Preference Fine-Tuning, RLHF/DPO 등): 편향·유해성·잘못된 정보 문제를 해결하는 단계
        
        - 사람이 평가한 ‘더 좋은/바람직한 응답’을 기준으로 모델 행동을 조정
        - 위험하거나 편향된 답변은 점수를 낮게 줘서 억제, 안전하고 사실 기반, 사람 친화적인 응답을 선택하도록 학습
        - 즉, 모델을 “안전하고 책임감 있게” 만들기 위한 단계
        </aside>
        
    - 4 지도 미세 조정(Supervised Finetuning, SFT)이 필요한 이유와, SFT를 위한 고품질 데모 데이터(Demonstration Data)를 구축할 때 고려해야 할 두 가지 주요 사항을 설명하세요.
        
        사전 학습된 모델의 문제점을 보완하기 위해 필요함. 특히, 단어 완성에 최적화 되어 있는 모델을 고품질 지시 데이터로 학습시켜 적절한 응답을 생성하도록 유도하기 위해 필요하다. 
        
        고품질 데모 데이터를 구축할 때 고려해야 할 두 가지 주요 사항 
        
        1. 질의 응답, 요약, 번역과 같이 모델이 처리하기 원하는 모든 요청 범위를 포함해야한다. (업무 예시를 써줄것)
        2. 비판적 사고, 정보 수집, 사용자의 요청의 적절성에 대한 판단이 필요한 복잡한 프롬프트가 포함될 수 있다.(단순 답변을 넘어서 사람처럼 생각하거 답할 수 있게)
    - 5 선호도 미세 조정(Preference Finetuning)의 초기 성공적인 알고리즘인 RLHF(Reinforcement Learning from Human Feedback)의 두 가지 주요 구성 요소와 목표를 설명하세요.
        
        선호도 파인튜닝의 목표: AI 모델이 사람의 선호도에 따라 행동하도록 만드는 것 
        
        RLHF의 구성 요소
        
        1. 파운 데이션 모델의 출력에 점수를 매기는 보상 모델을 학습한다.
        2. 보상 모델이 최대 점수를 줄 응답을 생성하도록 파운데이션 모델을 최적화한다. 
    - 6 언어 모델에서 '탐욕적 샘플링(Greedy Sampling)' 방식이 분류(Classification) 작업에는 효과적일 수 있으나, 텍스트 생성 작업에는 적합하지 않은 이유를 비교하여 설명하세요.
        
        Greedy sampling: 항상 가능성이 높은 결과를 선택하는 샘플링 방법 
        1. 언어 모델의 경우, 어떤 질문을 하든 가장 확률이 높은 단어를 고르게 된다면 답변이 매번 비슷하게 될것.
        
        2. 분류 모델의 경우, 하나의 정답을 골라내는 것이기 때문에 가능성이 높은 결과를 선택하는 것이 효과적이다.
        
    - 7 언어 모델에서 사용하는 토큰 샘플링 전략 중, Top-K 샘플링과 Top-P 샘플링이 작동하는 방식과 각각의 이점을 비교 설명하세요.
        
        Top-K 샘플링 :확률이 가장 높은 **상위** *K***개의 토큰**만 선택하여 샘플링.선택되는 토큰의 개수(*K*)가 **고정**됨.계산 워크로드 감소
        
        - **K 값**이 작으면 → 모델이 덜 창의적이고 예측 가능한 말을 많이 함
        - K 값이 크면 → 선택지가 많아져서 다양하고 창의적인 말이 나올 수 있음
        
        Top-P 샘플링 :확률을 누적하여 합(P)이 임계값 *P*를 초과하는 최소한의 토큰 집합을 동적으로 선택하여 샘플링.선택되는 토큰의 개수가 문맥에 따라 동적으로 변함.출력의 문맥적 적절성 향상.
        
        Top-P 샘플링은 실제로 잘 작동하는 것으로 입증되어 인기가 높아지고 있는 전략입니다. 이러한 샘플링 전략들은 모델의 출력을 제어하고 예측 가능성(낮은 온도, Top-K/P)과 창의성(높은 온도, Top-K/P) 사이의 균형을 맞추는 데 사용
        
    - 8 AI 모델이 본질적으로 '확률적(Probabilistic)'이라는 특성이 창의적인 작업과 일반적인 작업에 미치는 긍정적 및 부정적 영향을 각각 설명하세요.
        
        확률적(Probabilistic)이라는 특성은 모델이 응답을 생성하는 방식의 핵심이며, 이는 AI 응용 프로그램의 성능과 신뢰성에 광범위한 영향을 미침.
        
        창의적인 작업: 창의성 및 다양성 증대.무궁무진한 아이디어를 구상하고 이전에 본 적 없는 디자인을 생성/ 출력이 너무 무작위적이거나 일관성이 떨어질 수 있음
        
        일반적인 작업:무한한 출력 생성 능력.이는 번역, 요약, 코딩과 같은 작업들을 완성/일관성부족. 약간 다른 입력에 대해 극적으로 다른 대답 생성. 할루시네이션(사실이 아닌 내용을 지어냄)
        
        확률적 특성은 창의적인 응용 분야에서는 강력한 이점으로 작용하여 모델의 독창성과 다양성을 높이지만, 사실성 및 신뢰성이 요구되는 일반적인 응용 분야에서는 환각 및 일관성 부족이라는 주요 문제를 야기.
        
    - 9 AI 모델의 확률적 특성으로 인해 발생하는 두 가지 주요 문제(비일관성 및 환각)를 정의하고, 이들이 사용자 경험에 미치는 부정적인 영향을 설명하세요.
        - 비일관성
            - 같은 입력 또는 비슷한 입력에 대해 파운데이션 모델이 다른 출력을 내는 것
            - 이는 일관된 답변을 필요로 하는 사용자들에게 불편함을 줄 수 있다. 예를 들어, LLM을 활용하여 학생의 답을 평가하려고 하는데, 같은 입력 또는 비슷한 입력에 대해 LLM의 평가가 상이하면 일관된 평가를 하는데 문제가 발생한다.
        - 환각
            - LLM이 사실과 다른 답변을 마치 사실인 것처럼 답변하는 현상
            - 창의성이 중요하지 않고 정확한 사실이 중요한 작업의 경우 환각 문제는 사용자에게 치명적인 문제를 안겨줄 수 있다. 예를 들어 법률 관련 작업물에 환각이 발생하고 이를 그대로 사용하면 경우에 따라 법적인 처벌을 받거나 재판에 불리해질 수 있다.
    - 10 환각(Hallucination)이 발생하는 근본적인 원인 두 가지를 설명하고, 환각을 완화하기 위해 제시된 딥마인드(DeepMind)의 두 가지 기술적 접근 방식을 설명하세요.
        - **가설1. 자기 기만 가설** - 모델이 입력 데이터(프롬프트)와 자신이 생성한 데이터(predicted tokens)를 구분하지 못해서 환각이 발생할 수 있다. 만약 입력 데이터가 사실이고, 모델이 생성한 시퀀스가 거짓인 경우 모델은 다음 토큰을 예측할 때 자신이 이전에 생성한 토큰이 마치 사실인 것처럼 가정해서 다음 토큰을 생성한다. 이러한 과정에서 환각이 발생할 수 있다.
            
            **해결방법 제안**
            
            1. 강화 학습을 통해 모델이 사용자가 제시한 입력 프롬프트와 모델이 생성한 시퀀스를 구분할 수 있도록 학습한다.
            2. SFT과정에서 학습 데이터에 사실과 반사실적 데이터를 포함한다.
        - **가설2. 내부 지식 불일치 가설** - 모델이 학습한 내부 지식과 레이블러의 지식이 불일치하는 경우 환각이 발생할 수 있다. 예를 들어, SFT 과정에서 레이블러가 맞다고 생각하는 지식을 모델이 사전 학습 과정에서 학습하지 않은 경우, 파인튜닝 과정에서 모델에게 알고 있지 않은 답변을 하도록 훈련시키는 것과 같은 문제가 발생한다(환각을 발생시키도록 훈련하는 것처럼 보임).
            
            **해결방법 제안**
            
            1. 검증 - 각 응답에 대해 모델이 응답의 근거를 출처로 표시하도록 요청
            2. 보상 모델을 사용한 강화학습 - 환각을 발생시킬 때 더 큰 불이익을 주는 보상 모델로 모델을 학습
        - 추가적으로 신뢰할만한 외부 데이터베이스의 문서를 context로 사용하는 RAG나 프롬프트 엔지니어링 기법을 사용하여 환각을 완화시킬 수 있다. 4장에서 환각을 어떻게 감지하고 측정하는지 설명한다고 한다.
    - 11 AI 애플리케이션에서 구조화된 출력(Structured Outputs)이 필요한 두 가지 주요 시나리오를 제시하고, 프롬프팅(Prompting)이 구조화된 출력을 보장하지 못하는 근본적인 이유를 설명하세요.
        1. **시멘틱 파싱**:  출력 자체를  컴퓨터가 이해할 수 있는 형식으로 구조화
            
            **예시 1: SQL 쿼리로 변환**
            
            - 자연어: "2020년에 출시된 모든 영화를 찾아줘"
            - 구조화된 출력:
            
            ```sql
            SELECT * FROM movies
            WHERE release_year = 2020
            ```
            
            **예시 2: AI 에이전트의 MCP 함수 호출**
            
            1. **자연어 입력**
                - 사용자: "내 구글 드라이브에서 'Q3 보고서' 파일 찾아줘"
            2. **시맨틱 파싱 (의도 파악 및 구조화)**
                
                ```json
                {
                  "tool": "google_drive_search",
                  "parameters": {
                    "query": "Q3 보고서"
                  }
                }
                ```
                
            3. **MCP 호출 실행**
                - 파싱된 구조화된 명령을 실제 MCP 서버로 전달
            4. **결과 반환 및 응답 생성**
            
        2. 다운스트림 애플리케이션에서 사용: 다른 프로그램에서 사용되도록 출력을 구조화 
            
            **사용자 질문**: "이번 달 매출 상위 3개 제품은?"
            
            **자연어 출력**:
            
            ```
            이번 달 매출 상위 제품은 다음과 같습니다:
            1. 노트북 - 약 500만원
            2. 스마트폰 - 대략 450만원
            3. 태블릿 - 450만원 정도입니다.
            ```
            
            ### 해결: 구조화된 출력
            
            **구조화된 JSON 출력**:
            
            ```json
            {
              "top_products": [
                {"rank": 1, "name": "노트북", "revenue": 5000000},
                {"rank": 2, "name": "스마트폰", "revenue": 4500000},
                {"rank": 3, "name": "태블릿", "revenue": 4500000}
              ],
              "currency": "KRW",
              "period": "2025-11"
            }
            ```
            
            **시멘틱 파싱 VS 다운스트림 애플리케이션 사용**
            
            시맨틱 파싱: 입력 → 실행
            
            - 목적: 자연어를 즉시 실행 가능한 명령으로 변환
            
            다운스트림 사용: 출력 → 전달
            
            목적: AI의 분석/생성 결과를 다른 시스템이 사용하도록 구조화
            
    - 12 구조화된 출력을 보장하기 위한 네 가지 주요 접근 방식(프롬프팅, 제약된 샘플링, 테스트 시 컴퓨팅, 미세 조정) 중, 제약된 샘플링(Constrained Sampling)의 작동 방식을 로짓 벡터(Logit Vector) 개념을 활용하여 설명하세요.
        
        **로짓 벡터**: 모델이 다음 토큰을 생성하기 전 각 토큰에 대해 계산한 원시 점수(raw score)를 담은 벡터, 가장 점수가 높은 토큰이 선택됨.
        **제약된 샘플링**: 모델이 다음 토큰을 선택하기 전 토큰이 선택될 기준(필터)를 통과하지 못한 토큰은 점수를 크게 낮춤.
         
        
        **현재까지 생성된 텍스트**
        
        `{"name":`
        
        **모델이 계산한 원본 로짓 (일부만 표시)**
        
        | 토큰 | 로짓 값 | 설명 |
        | --- | --- | --- |
        | `"` | 12.5 | 문자열 시작 (JSON 규칙상 올바름) |
        | `123` | 8.2 | 숫자 (JSON 규칙 위반!) |
        | `{` | 6.1 | 객체 시작 (JSON 규칙 위반!) |
        | `null` | 5.8 | null 값 (JSON 규칙상 가능하지만 부적절) |
        | `true` | 4.2 | boolean (JSON 규칙 위반!) |
        | `홍길동` | 9.5 | 따옴표 없는 문자열 (JSON 규칙 위반!) |
        
        **필터링 후 로짓**
        
        | 토큰 | 원본 로짓 | 필터링 후 | 상태 |
        | --- | --- | --- | --- |
        | `"` | 12.5 | 12.5 | ✓ 허용 |
        | `123` | 8.2 | -∞ | 마스킹 |
        | `{` | 6.1 | -∞ | 마스킹 |
        | `null` | 5.8 | -∞ | 마스킹 |
        | `true` | 4.2 | -∞ | 마스킹 |
        | `홍길동` | 9.5 | -∞ | 마스킹 |
        
        **최종 결과**
        
        선택된 토큰: `"` (확률 100%)
        
    - 13 모델 학습 시 발생하는 메모리 병목 현상(Memory Bottlenecks)을 해결하기 위한 '양자화(Quantization)' 기법의 정의와, 이것이 메모리 및 계산 속도에 미치는 영향을 설명하세요.
        
        <aside>
        ✔️
        
        파라미터의 일부 소수점을 버려 정확도를 포기하는 대신 속도와 경량화를 추구함. 파라미터가 조단위 이상으로 커지는 이 시점에 양자화를 통한 경량화는 필수적이며 PEFT와 함께 고성능GPU가 아니더라도 모델링을 할 수 있게 기여함.
        
        </aside>
        
        <aside>
        ✅
        
        **정의:**
        
        양자화(Quantization)는 **모델의 값을 더 낮은 정밀도 형식(lower-precision format)으로 변환**하는 모든 기술을 포괄하는 용어입니다. 엄밀히 말해 정수 형식으로 변환하는 것만 양자화이지만, 실제로는 FP32(32비트)를 FP16, BF16, 또는 8비트 정수 등으로 변환하는 것을 의미합니다.
        
        **영향:**
        
        1. **메모리 절감:** 파라미터당 필요한 비트 수를 줄여서 모델의 메모리 점유 공간(memory footprint)이 비례적으로 감소합니다. 예를 들어, 16비트에서 8비트로 양자화하면 메모리 요구량이 절반으로 줄어듭니다. 이는 특히 훈련 시 모델 가중치뿐만 아니라 경사(gradient) 및 옵티마이저 상태(optimizer states)까지 저장해야 할 때 메모리 병목 현상을 완화하는 데 매우 중요합니다.
        2. **계산 속도 향상:** 정밀도가 낮아지면 데이터를 처리하는 데 필요한 시간이 줄어들어 **계산 속도가 빨라집니다**. 이는 추론 지연 시간(inference latency)과 훈련 시간을 단축시키는 효과를 가져옵니다. 또한, 메모리 절감을 통해 더 큰 배치 크기(batch size)를 허용하여 모델이 더 많은 입력을 병렬로 처리할 수 있게 됩니다.
        </aside>
        
    - 14 BF16(Bfloat16)과 FP16(Float16)이 모두 16비트를 사용함에도 불구하고, BF16이 FP16보다 범위(Range)는 더 크고 정밀도(Precision)는 낮은 이유를 해당 포맷의 비트 구성을 바탕으로 설명하세요.
        
        <aside>
        ✔️
        
        ? 이런게있엇어?
        
        </aside>
        
        <aside>
        ✅
        
        - **BF16 (Bfloat16):** BF16은 **지수부(범위)**에 더 많은 비트(8비트)를 할당하고, **가수부(정밀도)**에 더 적은 비트(7비트)를 할당합니다. 따라서 FP16이 표현할 수 없는 **더 큰 값**들을 표현할 수 있어 범위가 넓습니다. 하지만 가수부 비트가 적기 때문에 **정밀도(precision)는 낮습니다**.
        - **FP16 (Float16):** FP16은 **지수부**에 5비트를 할당하고, **가수부**에 10비트를 할당합니다. 이는 BF16보다 더 많은 비트를 정밀도에 사용하여 더 정밀하게 숫자를 표현할 수 있지만, 지수부 비트가 적어 **표현할 수 있는 값의 범위는 더 좁습니다**. 예를 들어, FP16에서는 범위를 벗어나는 큰 값들이 무한대(INF)로 반올림될 수 있습니다.
            
            ![image.png](AI%20%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81%20%EC%8A%A4%ED%84%B0%EB%94%94/image%201.png)
            
        </aside>
        
    - ~~15 크로스 엔트로피(Cross Entropy)와 퍼플렉시티(Perplexity)가 언어 모델 평가에 사용되는 방식에서, 퍼플렉시티를 낮추는 것이 모델 성능에 긍정적인 영향을 미치는 이유를 '불확실성'의 개념을 활용하여 설명하세요.~~
        
        <aside>
        ✔️
        
        ? 이런게잇었어2
        
        </aside>
        
        <aside>
        ✅
        
        - **크로스 엔트로피와 퍼플렉시티의 관계:**
        언어 모델은 훈련 데이터를 기반으로 해당 데이터의 확률 분포를 학습하고, 크로스 엔트로피(Cross Entropy)를 최소화하도록 훈련됩니다. **퍼플렉시티(Perplexity, PPL)**는 크로스 엔트로피의 지수 함수(exponentiation) 형태로, 이는 **모델이 텍스트를 예측할 때 느끼는 '불확실성(uncertainty)'의 척도**로 사용됩니다.
        - **성능과의 관계:**
        퍼플렉시티는 모델이 다음 토큰을 예측할 때 평균적으로 선택해야 하는 **균등하게 가능한 선택지(unique values)**의 개수를 나타냅니다. 따라서:
            1. **PPL이 높다**는 것은 모델의 예측이 **불확실하고**, 다음 토큰에 대해 고려해야 할 가능한 선택지가 많다는 것을 의미합니다.
            2. **PPL이 낮다**는 것은 모델의 예측이 **더 확실하고**, 훈련 데이터를 더 잘 학습했음을 의미합니다.
        
        퍼플렉시티가 낮을수록 모델은 **주어진 문맥에서 다음 토큰을 더 정확하게 예측할 가능성이 높아지며**, 이는 하류 작업(downstream tasks) 성능 향상으로 이어지는 중요한 대리 지표가 됩니다.
        
        </aside>
        
    - 16 언어 모델의 출력 확률을 나타내는 로그 확률(Logprobs)이 애플리케이션 구축 및 평가에 유용하지만, 상용 모델 제공업체들이 Logprobs API 노출을 제한하는 이유를 설명하세요.
        
        <aside>
        ✔️
        
        각 단어의 예측 확률을 표현하면 그 자체로 모델의 추론방향성이 노출되기때문
        
        </aside>
        
        <aside>
        ✅
        
        로그 확률(Logprobs)은 모델이 생성한 각 토큰에 대해 모델이 얼마나 **확신(confidence)하는지**를 측정하는 데 사용됩니다. 이는 다음과 같은 이유로 유용합니다:
        
        1. **평가 및 디버깅:** 모델의 유창성(fluency)이나 사실적 일관성(factual consistency)과 같은 측정을 위해 텍스트의 퍼플렉시티를 평가하는 데 사용될 수 있습니다.
        2. **분류 작업의 확신도:** 분류 작업에서 모델이 특정 클래스에 대해 높은 로그 확률을 보인다면, 이는 해당 예측에 대한 모델의 높은 확신도를 나타냅니다.
        3. **애플리케이션 구축:** 로그 확률을 사용하여 다중 출력 중 가장 확률이 높은 시퀀스를 선택하는 등 테스트 시 컴퓨팅(Test Time Compute) 기법에 활용됩니다.
        
        **API 노출 제한 이유:**
        
        대부분의 모델 제공업체(예: Anthropic, OpenAI)가 로그 확률 API 노출을 제한하거나 그 범위를 축소하는 주된 이유는 **보안(security) 문제**와 관련이 있습니다.
        
        - **모델 복제 위험:** 모델이 노출하는 로그 확률 정보를 활용하면, 외부에서 해당 모델의 내부 작동 방식과 통계적 지식을 더 쉽게 파악하고 **모델을 복제(replicate)하거나 모방(mimic)하는 것이 용이**해질 수 있습니다.
        </aside>
        
    
    ---
    
    ## 📚 예시 답안
    
    1. **모델 규모(Model Size)를 나타내는 세 가지 핵심 지표(파라미터 수, 학습 토큰 수, FLOPs)와 각각이 모델의 어떤 측면을 대변하는지 설명하세요.**
    
    세 가지 핵심 지표와 그것이 대변하는 모델의 측면은 다음과 같습니다:
    
    - **파라미터 수 (Number of parameters):** 이 지표는 **모델의 학습 능력(learning capacity) 또는 잠재력**을 나타내는 대리 지표입니다. 파라미터 수가 많을수록 모델은 더 복잡한 패턴과 지식을 저장할 수 있습니다.
    - **학습 토큰 수 (Number of training tokens):** 이 지표는 모델이 **얼마나 많은 양의 정보를 학습했는지**를 나타내는 대리 지표입니다. 예를 들어, 데이터셋에 1조 개의 토큰이 있고 모델이 이를 두 번 학습했다면 학습 토큰 수는 2조 개가 됩니다.
    - **FLOPs (Floating Point Operations):** 이 지표는 모델 학습에 필요한 **계산량 또는 훈련 비용**을 나타내는 대리 지표입니다. 이는 모델을 훈련하는 데 필요한 컴퓨팅 자원의 양을 측정하는 데 사용됩니다.
    1. **모델 개발 예산이 고정되어 있을 때, 컴퓨팅 최적 모델(Compute-Optimal Model)을 구축하기 위한 스케일링 법칙(Scaling Law)의 핵심 원칙을 설명하세요.**
    
    컴퓨팅 최적 모델은 고정된 컴퓨팅 예산(FLOPs) 내에서 **최상의 성능**을 달성할 수 있는 모델 크기(파라미터 수)와 데이터셋 크기(토큰 수)의 조합을 찾는 것을 목표로 합니다. 스케일링 법칙은 이 세 가지 요소(성능, 모델 크기, 데이터 크기) 사이의 관계를 나타냅니다.
    
    핵심 원칙은 다음과 같습니다:
    
    - **자원 균형의 중요성:** 스케일링 법칙에 따르면, 제한된 컴퓨팅 예산 내에서 모델 성능을 최대화하려면 **파라미터 수와 학습 토큰 수를 균형 있게 증가**시켜야 합니다.
    - **컴퓨팅 최적 비율:** 과거에는 모델 크기를 먼저 키우고 나서 데이터를 채우는 경향이 있었지만, 스케일링 법칙은 특정 예산에서 **파라미터 수와 토큰 수에 할당되는 자원이 모두 최적화**되어야 함을 제시합니다. 이는 모델이 더 많은 데이터를 볼수록 성능이 향상되며, 데이터의 양도 모델 크기만큼 중요함을 의미합니다.
    1. **사전 훈련된 모델(Pre-trained Model)이 일반적으로 가지는 두 가지 주요 문제점을 설명하고, 사후 훈련(Post-Training)이 이 문제들을 어떻게 해결하는지 설명하세요.**
    
    사전 훈련(Pre-training)은 주로 자기 지도 학습(self-supervision) 방식으로 이루어지며, 모델은 이 과정에서 방대한 세계 지식을 습득하지만 두 가지 주요 문제점을 가집니다:
    
    1. **텍스트 완성(Completion)에 최적화된 행동:** 자기 지도 학습은 모델을 텍스트 완성(다음 토큰 예측)에 최적화시키기 때문에, 사용자가 대화(Conversing)나 특정 지침을 따르도록 요청했을 때 모델은 적절한 응답 대신 문장을 완성하려 할 수 있습니다.
    2. **안전하지 않거나 편향된 출력:** 사전 훈련 데이터가 인터넷에서 무차별적으로 수집된 경우, 모델의 출력은 인종차별적이거나, 성차별적이거나, 무례하거나, 또는 단순히 잘못된 내용(hallucinations)을 포함할 수 있습니다.
    
    **사후 훈련(Post-Training)을 통한 해결:**
    
    사후 훈련은 **모델을 인간의 선호도(human preferences)에 맞춰 정렬(align)**시키는 것을 목표로 합니다.
    
    - **문제 1 해결 (행동 정렬):** 지도 미세 조정(SFT)을 통해 모델에 **명시적인 지침(instruction)과 응답 쌍**을 학습시켜, 단순히 텍스트를 완성하는 것이 아니라 사용자의 요청에 적절하게 응답하는 방법을 가르칩니다.
    - **문제 2 해결 (안전 및 정렬):** 선호도 미세 조정(Preference Finetuning)을 통해 인간의 선호도(안전, 유용성 등)를 반영하는 보상 모델(Reward Model)을 훈련하고, 이를 바탕으로 모델이 유해하거나 편향된 응답을 피하도록 최적화합니다.
    1. **지도 미세 조정(Supervised Finetuning, SFT)이 필요한 이유와, SFT를 위한 고품질 데모 데이터(Demonstration Data)를 구축할 때 고려해야 할 두 가지 주요 사항을 설명하세요.**
    
    **SFT가 필요한 이유:**
    
    사전 훈련된 모델은 일반적으로 텍스트 완성(completion)에 최적화되어 있으므로, 사용자의 요청을 대화(conversing)나 질문/답변으로 인식하지 못합니다. SFT는 고품질의 주석이 달린 데이터(annotated data)를 사용하여 모델을 인간의 사용 패턴과 선호도에 맞춰 **행동을 다듬고 정렬**하는 데 필수적입니다. SFT를 통해 모델은 질문-답변, 요약, 번역 등 다양한 유형의 요청을 처리하는 방법을 학습합니다.
    
    **고품질 데모 데이터 구축 시 고려 사항 (두 가지):**
    
    1. **다양한 요청 유형의 포괄성 (Coverage):** 모델이 처리해야 할 모든 요청 유형(예: 질문 응답, 요약, 번역, 분류 등)을 데모 데이터가 포함해야 합니다. 요청 유형이 다양할수록 모델은 광범위한 작업에 효과적으로 응답하는 방법을 학습합니다.
    2. **주석 품질 및 전문성 (Quality & Expertise):** 데모 데이터의 응답은 비판적 사고, 정보 수집, 사용자 요청의 적절성에 대한 판단을 요구할 수 있으므로, **고품질의 주석 작업자(labelers) 또는 도메인 전문가**가 필요합니다. 이는 일반적인 데이터 라벨링과 달리 복잡한 지침에 대한 적절한 응답을 생성해야 하기 때문입니다.
    3. **선호도 미세 조정(Preference Finetuning)의 초기 성공적인 알고리즘인 RLHF(Reinforcement Learning from Human Feedback)의 두 가지 주요 구성 요소와 목표를 설명하세요.**
    
    RLHF (Reinforcement Learning from Human Feedback)는 사전 훈련 및 지도 미세 조정 이후에 모델을 인간의 선호도에 맞추기 위해 사용되는 알고리즘입니다.
    
    **두 가지 주요 구성 요소:**
    
    1. **보상 모델 훈련 (Reward Model Training):** 이 모델은 **파운데이션 모델의 출력에 점수를 매기는 역할**을 합니다. 인간 주석 작업자(labelers)에게 두 개 이상의 응답을 비교하여 선호도를 순위로 매기도록 요청한 비교 데이터(comparison data)를 사용하여 훈련됩니다.
    2. **파운데이션 모델 최적화 (Foundation Model Optimization):** 보상 모델이 부여하는 **점수(reward)를 최대화하도록 파운데이션 모델을 최적화**합니다. 이를 통해 모델은 인간이 선호하는 응답을 생성하도록 학습됩니다.
    
    **목표:**
    
    RLHF의 궁극적인 목표는 **모델을 인간의 선호도와 의도에 밀접하게 정렬**하여, 모델이 더 유용하고 안전하며 지침을 잘 따르는 응답을 생성하도록 만드는 것입니다.
    
    1. **언어 모델에서 '탐욕적 샘플링(Greedy Sampling)' 방식이 분류(Classification) 작업에는 효과적일 수 있으나, 텍스트 생성 작업에는 적합하지 않은 이유를 비교하여 설명하세요.**
    - **분류(Classification) 작업에서의 효과:**
    분류 작업(예: 스팸 탐지)에서는 가능한 결과가 고정되어 있으며, 가장 높은 확률을 가진 결과(예: 스팸일 확률 90%)를 선택하는 것이 합리적이고 정확합니다. 탐욕적 샘플링은 **가장 확률이 높은 단일 결과를 항상 선택**하기 때문에, 결정론적이고 일관된 결과가 필요한 분류 작업에 적합합니다.
    - **텍스트 생성(Text Generation) 작업에서의 부적합성:**
    텍스트 생성 작업에서 탐욕적 샘플링을 사용하면 모델은 매 단계에서 **가장 흔하거나 예측 가능한 토큰만 반복적으로 선택**하게 됩니다. 이는 다음 토큰 예측이 단기적으로는 최적일 수 있지만, 전체 시퀀스로 봤을 때는 반복적이고 **지루하며 창의성이 없는 출력**을 생성하게 만듭니다. 생성 AI의 중요한 속성인 다양성과 창의성을 떨어뜨리기 때문에 생성 작업에는 적합하지 않습니다.
    1. **언어 모델에서 사용하는 토큰 샘플링 전략 중, Top-K 샘플링과 Top-P 샘플링이 작동하는 방식과 각각의 이점을 비교 설명하세요.**
    
    언어 모델은 다음 토큰을 생성하기 위해 어휘집(vocabulary) 전체에 대한 확률 분포를 계산합니다. 이 분포에서 어떤 토큰을 선택할지 결정하는 것이 샘플링 전략입니다.
    
    | 구분 | 작동 방식 | 이점 |
    | --- | --- | --- |
    | **Top-K 샘플링** | **확률이 가장 높은 상위 K개의 토큰**을 미리 선택하고, 선택된 이 K개 토큰들 중에서만 샘플링을 수행합니다. K 값은 고정된 숫자입니다 (예: 50~500). | 1. **계산량 감소:** 상위 K개의 로짓에 대해서만 소프트맥스(softmax)를 수행하여 계산 부하를 줄일 수 있습니다. 2. **예측 가능성 제어:** K 값이 작을수록 텍스트가 더 예측 가능하고 일관성이 높아집니다. |
    | **Top-P 샘플링** | **누적 확률이 P(%)를 초과하는 최소한의 토큰 집합**을 선택하고, 이 토큰 집합 내에서만 샘플링을 수행합니다. P 값은 고정된 백분율입니다 (예: 90%). | 1. **상황 적합성 개선:** 각 문맥에 따라 관련성 높은 토큰의 개수가 유동적으로 변하기 때문에, **출력이 문맥적으로 더 적절**하게 됩니다. 2. **다양성 유지:** 문맥상 매우 다양한 토큰들이 높은 확률을 가질 때는 더 많은 토큰을 고려할 수 있습니다. |
    1. **AI 모델이 본질적으로 '확률적(Probabilistic)'이라는 특성이 창의적인 작업과 일반적인 작업에 미치는 긍정적 및 부정적 영향을 각각 설명하세요.**
    
    AI 모델은 출력을 생성할 때 확률을 기반으로 샘플링하기 때문에, 같은 질문에도 매번 다른 답변을 생성할 수 있습니다.
    
    **1. 창의적인 작업에 미치는 긍정적 영향:**
    
    - **창의성 극대화:** 확률적 특성은 모델이 일반적인 경로를 넘어 탐색하고 "상자 밖에서 생각"할 수 있게 합니다.
    - **아이디어 생성:** 제한 없는 아이디어를 생성하고, 이전에 본 적 없는 디자인을 만들어내는 등 창의적인 전문가의 훌륭한 조력자 역할을 합니다.
    
    **2. 일반적인 작업에 미치는 부정적 영향:**
    
    - **불일치성(Inconsistency):** 반복적으로 동일한 질문을 했을 때 모델의 답변이 달라지면 (예: 에세이 점수를 다르게 매기는 경우), 사용자는 혼란을 느끼고 시스템에 대한 신뢰도가 떨어지게 됩니다.
    - **환각(Hallucination):** 비록 확률이 0이 아니면 아무리 터무니없거나 잘못된 내용이라도 AI에 의해 생성될 수 있으며, 사실 관계를 기반으로 해야 하는 작업(예: 법률 조사, 의학 정보)에서는 치명적인 오류와 피해를 초래할 수 있습니다.
    1. **AI 모델의 확률적 특성으로 인해 발생하는 두 가지 주요 문제(불일치 및 환각)를 정의하고, 이들이 사용자 경험에 미치는 부정적인 영향을 설명하세요.**
    
    **1. 불일치성 (Inconsistency):**
    
    - **정의:** 동일한 입력(프롬프트)에 대해 AI 모델이 실행할 때마다 다른 출력을 생성하는 현상입니다.
    - **부정적 영향:** 인간은 AI와의 상호 작용에서도 일관성을 기대하는데, 불일치성은 **사용자 경험에 혼란**을 야기하고, AI 시스템이 **신뢰할 수 없다**는 인상을 주어 사용자가 모델을 잘못된 응답을 제공하는 "말 그대로 다른 사람"처럼 느끼게 만듭니다.
    
    **2. 환각 (Hallucination):**
    
    - **정의:** AI 모델이 사실과 일치하지 않거나, 주어진 입력이나 지식 기반으로 뒷받침되지 않는 정보를 자신 있게 만들어내는 현상입니다.
    - **부정적 영향:** 환각은 **사실성(factuality)에 의존하는 작업에 치명적**입니다. 사용자가 법률이나 의학 같은 고위험 분야에서 부정확하거나 거짓된 정보를 신뢰하게 만들어 실제 소송 패배나 위험한 행동 유도로 이어지는 등 **재앙적인 실패**를 초래할 수 있습니다.
    1. **환각(Hallucination)이 발생하는 근본적인 원인 두 가지를 설명하고, 환각을 완화하기 위해 제시된 딥마인드(DeepMind)의 두 가지 기술적 접근 방식을 설명하세요.**
    
    **환각의 근본적인 원인 (두 가지):**
    
    1. **훈련 데이터의 제약 (Training Data Limitations):** 모델이 훈련 데이터 내의 패턴에 지나치게 의존하거나, 데이터가 부족하거나 (특히 희소하거나 새로운 정보의 경우), 혹은 훈련 데이터 자체가 오류나 허위 정보를 포함할 때 환각이 발생할 수 있습니다.
    2. **확률 기반의 생성 과정 (Probabilistic Generation):** 언어 모델은 다음 토큰을 예측할 때 가장 그럴듯한(plausible) 시퀀스를 생성하도록 최적화되어 있지, **사실적으로 정확한(factually correct) 시퀀스를 생성하도록 보장되지는 않습니다**. 낮은 확률이라도 사실과 다른 토큰을 생성할 수 있으며, 일단 잘못된 가정을 하면 그 이후의 토큰들은 이 가정에 근거하여 눈덩이처럼 오류를 키워나갈 수 있습니다.
    
    **환각 완화를 위한 딥마인드의 기술적 접근 방식 (두 가지):**
    
    1. **강화 학습 기법 활용:** 사용자 제공 프롬프트(관찰)와 모델이 생성한 토큰(행동)을 구분하도록 모델을 훈련시키는 강화 학습(Reinforcement Learning) 기법을 사용합니다.
    2. **지도 학습 기법 활용:** 훈련 데이터에 **사실적 신호(factual signals)와 반(反)사실적 신호(counterfactual signals)**를 모두 포함하여 모델을 지도 학습(Supervised Learning) 방식으로 훈련시킵니다.
    3. **AI 애플리케이션에서 구조화된 출력(Structured Outputs)이 필요한 두 가지 주요 시나리오를 제시하고, 프롬프팅(Prompting)이 구조화된 출력을 보장하지 못하는 근본적인 이유를 설명하세요.**
    
    **구조화된 출력이 필요한 두 가지 시나리오:**
    
    1. **정확한 응답 및 검증이 필수적인 작업:** 출력이 유효한 정규식(regex)이거나, 미리 정의된 유효한 클래스(valid classes) 중 하나여야 하는 분류(classification) 작업과 같이 **정확한 형식이 요구**되는 경우입니다.
    2. **다운스트림 애플리케이션 사용:** AI의 출력이 다른 애플리케이션(다운스트림 시스템)에 의해 사용될 때, 해당 애플리케이션이 **특정 형식(예: 특정 키를 가진 JSON 문서)**으로 입력을 파싱(parse)해야 하는 경우입니다.
    
    **프롬프팅이 보장하지 못하는 근본적인 이유:**
    
    모델의 **지침 준수 능력(instruction-following capability)에 전적으로 의존**하며, 이 능력은 모델 자체와 지침의 명확성에 따라 달라지기 때문입니다. 모델이 지침을 아무리 잘 따르더라도, **유효하지 않은 출력(invalid model outputs)이 몇 퍼센트라도 발생할 수 있으며**, 이는 많은 애플리케이션에서 허용될 수 없는 위험입니다. 즉, 프롬프팅만으로는 모델이 항상 지정된 문법이나 형식(예: 완벽한 JSON)을 따를 것이라는 **보장(guarantee)이 없습니다**.
    
    1. **구조화된 출력을 보장하기 위한 네 가지 주요 접근 방식(프롬프팅, 제약된 샘플링, 테스트 시 컴퓨팅, 미세 조정) 중, 제약된 샘플링(Constrained Sampling)의 작동 방식을 로짓 벡터(Logit Vector) 개념을 활용하여 설명하세요.**
    
    제약된 샘플링(Constraint Sampling)은 텍스트 생성을 특정 제약 조건에 맞게 유도하는 기술입니다.
    
    **작동 방식:**
    
    1. **로짓 벡터 생성:** 모델이 입력을 처리한 후, 어휘집 내의 모든 가능한 토큰에 해당하는 **로짓 벡터(Logit Vector)**를 출력합니다. 로짓 값은 확률에 해당하지만, 아직 확률 분포로 변환되지 않은 값입니다.
    2. **필터링 적용:** 제약된 샘플링은 이 로짓 벡터에 제약 조건(예: 출력은 '빨강', '파랑', '초록' 중 하나여야 함)을 적용하여, **제약 조건을 충족하는 토큰에 해당하는 로짓만 남기고 나머지는 필터링**합니다.
    3. **샘플링:** 필터링된 유효한 토큰들의 로짓을 사용하여 소프트맥스(Softmax)를 수행하고 확률을 계산한 후, **오직 이 유효한 토큰들 사이에서만 다음 토큰을 샘플링**합니다.
    4. **문법 준수:** 이 과정은 각 단계에서 어떤 토큰이 허용되는지 지정하는 문법(grammar, 예: JSON 문법)을 기반으로 이루어지며, 이를 통해 모델이 문법을 벗어난 토큰을 생성하는 것을 원천적으로 방지합니다.
    5. **모델 학습 시 발생하는 메모리 병목 현상(Memory Bottlenecks)을 해결하기 위한 '양자화(Quantization)' 기법의 정의와, 이것이 메모리 및 계산 속도에 미치는 영향을 설명하세요.**
    
    **정의:**
    
    양자화(Quantization)는 **모델의 값을 더 낮은 정밀도 형식(lower-precision format)으로 변환**하는 모든 기술을 포괄하는 용어입니다. 엄밀히 말해 정수 형식으로 변환하는 것만 양자화이지만, 실제로는 FP32(32비트)를 FP16, BF16, 또는 8비트 정수 등으로 변환하는 것을 의미합니다.
    
    **영향:**
    
    1. **메모리 절감:** 파라미터당 필요한 비트 수를 줄여서 모델의 메모리 점유 공간(memory footprint)이 비례적으로 감소합니다. 예를 들어, 16비트에서 8비트로 양자화하면 메모리 요구량이 절반으로 줄어듭니다. 이는 특히 훈련 시 모델 가중치뿐만 아니라 경사(gradient) 및 옵티마이저 상태(optimizer states)까지 저장해야 할 때 메모리 병목 현상을 완화하는 데 매우 중요합니다.
    2. **계산 속도 향상:** 정밀도가 낮아지면 데이터를 처리하는 데 필요한 시간이 줄어들어 **계산 속도가 빨라집니다**. 이는 추론 지연 시간(inference latency)과 훈련 시간을 단축시키는 효과를 가져옵니다. 또한, 메모리 절감을 통해 더 큰 배치 크기(batch size)를 허용하여 모델이 더 많은 입력을 병렬로 처리할 수 있게 됩니다.
    3. **BF16(Bfloat16)과 FP16(Float16)이 모두 16비트를 사용함에도 불구하고, BF16이 FP16보다 범위(Range)는 더 크고 정밀도(Precision)는 낮은 이유를 해당 포맷의 비트 구성을 바탕으로 설명하세요.**
    
    FP32를 포함하여 부동 소수점 형식(Floating-Point formats)은 일반적으로 부호(sign), 범위(range, 지수부), 정밀도(precision, 가수부)로 구성됩니다. BF16과 FP16은 모두 16비트를 사용하지만, 비트 할당 방식이 다릅니다.
    
    - **BF16 (Bfloat16):** BF16은 **지수부(범위)**에 더 많은 비트(8비트)를 할당하고, **가수부(정밀도)**에 더 적은 비트(7비트)를 할당합니다. 따라서 FP16이 표현할 수 없는 **더 큰 값**들을 표현할 수 있어 범위가 넓습니다. 하지만 가수부 비트가 적기 때문에 **정밀도(precision)는 낮습니다**.
    - **FP16 (Float16):** FP16은 **지수부**에 5비트를 할당하고, **가수부**에 10비트를 할당합니다. 이는 BF16보다 더 많은 비트를 정밀도에 사용하여 더 정밀하게 숫자를 표현할 수 있지만, 지수부 비트가 적어 **표현할 수 있는 값의 범위는 더 좁습니다**. 예를 들어, FP16에서는 범위를 벗어나는 큰 값들이 무한대(INF)로 반올림될 수 있습니다.
    1. **크로스 엔트로피(Cross Entropy)와 퍼플렉시티(Perplexity)가 언어 모델 평가에 사용되는 방식에서, 퍼플렉시티를 낮추는 것이 모델 성능에 긍정적인 영향을 미치는 이유를 '불확실성'의 개념을 활용하여 설명하세요.**
    - **크로스 엔트로피와 퍼플렉시티의 관계:**
    언어 모델은 훈련 데이터를 기반으로 해당 데이터의 확률 분포를 학습하고, 크로스 엔트로피(Cross Entropy)를 최소화하도록 훈련됩니다. **퍼플렉시티(Perplexity, PPL)**는 크로스 엔트로피의 지수 함수(exponentiation) 형태로, 이는 **모델이 텍스트를 예측할 때 느끼는 '불확실성(uncertainty)'의 척도**로 사용됩니다.
    - **성능과의 관계:**
    퍼플렉시티는 모델이 다음 토큰을 예측할 때 평균적으로 선택해야 하는 **균등하게 가능한 선택지(unique values)**의 개수를 나타냅니다. 따라서:
        1. **PPL이 높다**는 것은 모델의 예측이 **불확실하고**, 다음 토큰에 대해 고려해야 할 가능한 선택지가 많다는 것을 의미합니다.
        2. **PPL이 낮다**는 것은 모델의 예측이 **더 확실하고**, 훈련 데이터를 더 잘 학습했음을 의미합니다.
    
    퍼플렉시티가 낮을수록 모델은 **주어진 문맥에서 다음 토큰을 더 정확하게 예측할 가능성이 높아지며**, 이는 하류 작업(downstream tasks) 성능 향상으로 이어지는 중요한 대리 지표가 됩니다.
    
    1. **언어 모델의 출력 확률을 나타내는 로그 확률(Logprobs)이 애플리케이션 구축 및 평가에 유용하지만, 상용 모델 제공업체들이 Logprobs API 노출을 제한하는 이유를 설명하세요.**
    
    **Logprobs의 유용성:**
    
    로그 확률(Logprobs)은 모델이 생성한 각 토큰에 대해 모델이 얼마나 **확신(confidence)하는지**를 측정하는 데 사용됩니다. 이는 다음과 같은 이유로 유용합니다:
    
    1. **평가 및 디버깅:** 모델의 유창성(fluency)이나 사실적 일관성(factual consistency)과 같은 측정을 위해 텍스트의 퍼플렉시티를 평가하는 데 사용될 수 있습니다.
    2. **분류 작업의 확신도:** 분류 작업에서 모델이 특정 클래스에 대해 높은 로그 확률을 보인다면, 이는 해당 예측에 대한 모델의 높은 확신도를 나타냅니다.
    3. **애플리케이션 구축:** 로그 확률을 사용하여 다중 출력 중 가장 확률이 높은 시퀀스를 선택하는 등 테스트 시 컴퓨팅(Test Time Compute) 기법에 활용됩니다.
    
    **API 노출 제한 이유:**
    
    대부분의 모델 제공업체(예: Anthropic, OpenAI)가 로그 확률 API 노출을 제한하거나 그 범위를 축소하는 주된 이유는 **보안(security) 문제**와 관련이 있습니다.
    
    - **모델 복제 위험:** 모델이 노출하는 로그 확률 정보를 활용하면, 외부에서 해당 모델의 내부 작동 방식과 통계적 지식을 더 쉽게 파악하고 **모델을 복제(replicate)하거나 모방(mimic)하는 것이 용이**해질 수 있습니다.
