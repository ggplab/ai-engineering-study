# 04단원

    
    ### 💡 문제 출제
    
    - **문제 1 :** 모델의 도메인별 역량(Domain-Specific Capability)을 평가하는 데 있어, 대부분의 공개 벤치마크가 다지선다형 질문(MCQs)과 같은 폐쇄형 작업(close-ended tasks)을 선호하는 주된 이유는 무엇입니까?
        
        <aside>
        💡
        
        1. **용이한 검증 및 재현성:** MCQs는 모델의 응답이 예상되는 정답과 일치하는지 **쉽게 확인하고 재현**할 수 있게 해줍니다. 이는 자동화된 평가 시스템을 구축하는 데 유리
        
        2. **지식 및 추론 능력 평가:** MCQs는 모델이 특정 도메인에 대한 지식(knowledge)을 보유하고 있는지, 그리고 **추론(reasoning)** 능력을 가지고 있는지를 평가하는 데 가장 적합
        
        3. **무작위 기준선(Random Baseline) 대비 용이한 평가:** 질문이 네 가지 옵션 중 하나를 고르는 형태라면, 모델의 무작위 기준선 정확도는 25%가 됩니다. MCQs는 모델 성능을 이 **무작위 기준선과 비교하여 평가하기 쉬움**
        
        다만, 이러한 폐쇄형 벤치마크는 **생성 역량**을 평가하는 데는 이상적이지 않습니다. MCQs는 모델이 좋은 응답을 생성하는 능력이 아니라, 주어진 옵션 중에서 좋은 응답과 나쁜 응답을 구별하는 능력(분류)을 테스트하기 때문
        
        따라서, 수학, 과학 지식, 일반 상식, 법률 지식 등 **지식 기반(knowledge-based)** 또는 **추론 기반(reasoning-based)** 역량을 평가할 때는 MCQs가 널리 사용되지만, 요약, 번역, 에세이 작성과 같은 **생성 능력**을 평가할 때는 적합하지 않음
        
        </aside>
        
    - **문제 2:** 코드 생성 능력 평가에 사용되는 주요 지표인 Pass@k는 무엇을 의미하며, 이 점수가 k 값에 따라 어떻게 변화합니까?
        
        <aside>
        💡
        
        **Pass@k**는 코드 생성 능력 평가에 사용되는 주요 지표입니다.
        
        - **Pass@k**는 모델이 생성한 *k*개의 코드 샘플 중 **해당 문제의 모든 테스트 케이스(test case)를 통과한 문제의 비율**을 의미
        - 예를 들어, 10개의 문제가 있고 *k*=3으로 모델이 5문제를 해결했다면, 해당 모델의 Pass@3 점수는 50%가 됨
        - 코드 생성 능력과 같은 코딩 관련 역량은 일반적으로 기능적 정확성(Functional Correctness)을 사용하여 평가
        
        **k 값에 따른 점수 변화**
        
        - 모델이 생성하는 코드 샘플의 수, 즉 *k* 값이 많아질수록, 모델이 각 문제를 해결할 수 있는 가능성이 더 커짐
        - 따라서, 기대치(in expectation)로 볼 때 **Pass@k 점수는** *k* **값이 커질수록 더 높아져야 합니다**. 이는 Pass@1 점수가 Pass@3 점수보다 낮아야 하며, Pass@3 점수는 Pass@10 점수보다 낮아야 함을 의미
        </aside>
        
    - **문제 3 :** 자연어 생성(NLG) 작업에서 유창성(Fluency)과 일관성(Coherence)이 과거에 중요했지만, 오늘날 파운데이션 모델 평가에서 그 중요성이 낮아진 이유를 설명하십시오.
        
        <aside>
        💡
        
        </aside>
        
    - **문제 4 :** 사실적 일관성(Factual Consistency)을 검증하는 두 가지 설정, 지역 사실적 일관성(Local Factual Consistency)과 글로벌 사실적 일관성(Global Factual Consistency)의 차이점을 컨텍스트 기준으로 설명하십시오.
        
        <aside>
        💡
        
        국소적(지역) 사실 일관성(Local Factual Consistency)
        
        - AI가 답변을 만들 때 참고하라고 미리 제공한 자료 = 컨텍스트(context)에 의해 지지되는지를 평가함
        - 만약 컨텍스트 안의 내용과 다르면 사실적으로 *일관되지 않은(output inconsistent)* 것으로 봄
        - 예시) 컨텍스트: “하늘은 보라색이다.”
        
                        모델 출력: “하늘은 파랗다.”
        
                  → 컨텍스트와 모순 → 지역 사실 불일치❌ 
        
                        모델 출력: “하늘은 보라색이다.”
        
                  → 컨텍스트와 일치 → 지역 사실 일관성 ✔
        
        전역적(글로벌) 사실 일관성(Global Factual Consistency)
        
        - 모델의 출력이 “일반적으로 받아들여지는 사실(open knowledge)”과 같은지를 평가함
        - 컨텍스트가 없으면  외부 자료를 찾아 사실을 확인해야 하므로 사실 검증 과정이 더 어렵고 훨씬 복잡해짐
        - 예시) 모델 출력: “하늘은 파랗다.”
        
                        → 일반적으로 사실 → 글로벌 사실 일관성 ✔
        
        </aside>
        
    - **문제 5 :** 모델의 환각(Hallucination)을 측정하기 위한 두 가지 정교한 AI 심사위원 기술인 SelfCheckGPT와 SAFE(Search-Augmented Factuality Evaluator)의 기본 작동 원리를 비교 설명하십시오.
        
        <aside>
        💡
        
        SelfCheckGPT
        
        - 같은 질문을 모델에게 여러 번 물어보고 답변이 서로 얼마나 일치하는지 확인하는 방식
        - 만약 답변이 반복할 때마다 내용이 들쑥날쑥하다면,
            
            → 모델이 해당 사실을 정확히 모르고 추측했다는 신호
            
            → 즉, 환각(Hallucination) 가능성이 높다고 판단함
            
        - 외부 지식 없이 모델의 내부 일관성만으로 검증하는 방법
        
        SAFE (Search-Augmented Factuality Evaluator)
        
        - 모델의 답변을 작은 사실 단위로 쪼갠 뒤 각각을 확인하기 위해 검색 엔진에 쿼리를 보내고, 검색 결과를 바탕으로 “이 사실이 진짜 맞는지”를 AI가 다시 판정하는 구조
        
              즉, 모델이 말한 내용을 인터넷을 통해 직접 검증하는 방식
        
        - 외부 지식을 활용하기 때문에 정확도가 높은 반면 검색 과정이 있어 비용·시간이 증가함
        </aside>
        
    - **문제 6 :** 정치적 또는 종교적 편향(Biases toward a political or religious ideology)이 모델의 출력에 미치는 영향을 설명하십시오.
        
        <aside>
        💡
        
        AI 모델은 인터넷, 문서, SNS 등 사람이 만든 데이터를 기반으로 학습하기 때문에 그 데이터에 포함된 정치적·종교적 편향이 모델의 출력에도 영향을 주게 되고 모델의 중립성·공정성·신뢰성에을 해칠 수 있음
        
        ① 특정 정치·종교 관점을 더 긍정적으로 표현
        
        ② 논쟁적 주제에서 균형 잡힌 답변을 못할 수 있음
        
        ③ 사용자에게 왜곡된 세계관을 전달할 수 있음
        
        ④ 실제 모델별로 성향이 다를 수도 있음
        
        </aside>
        
    - **문제 7 :** Google의 IFEval (Instruction-Following Evaluation) 벤치마크가 명령어 따르기 능력을 평가하는 주요 방식은 무엇입니까?
        
        <aside>
        💡
        
        IFEval (Instruction-Following Evaluation)
        
        1. 평가 초점: 모델이 예상된 형식에 맞는 출력을 생성할 수 있는지에 초점을 맞춤
        2. 평가 도구: 자동으로 검증할 수 있는 25가지 유형의 지시를 정의했음 > 즉, 객관적인 기준을 만들어 사람의 주관적인 판단 없이 컴퓨터가 스스로, 자동으로 채점될 수 있게 테스트를 만든 것
        3. 평가 결과: 지시 이행률, 정확하게 따른 지시의 비율을 구체적으로 분석함 > 모델이 어떤 규칙에 강하고 어떤 규칙에 약한지를 파악(ex: 길이 제약에 강하지만, JSON 형식 생성에는 약함)
        </aside>
        
    - **문제 8 :** 모델 선택 시 모델 품질, 지연 시간(Latency), 그리고 비용(Cost)을 균형 있게 고려해야 하는 이유를 설명하십시오.
        
        <aside>
        💡
        
        이 세가지 요소는 트레이드 오프 관계에 잇기 때문에 ‘품질이 높음(좋은 결과물) > 지연시간이 늘어남 + 더 많은 컴퓨터 자원이 필요해서 비용이 늘어남’ 하나를 높이려면 다른 하나를 희생해야하는 경우가 많음. 
        
        비약적으로 비유해보자면 ‘박사채용 > 인건비가 너무 비싸고 채용에 시간이 오래걸림, 학사채용 > 박사 인력대비 많은 인력 + 비용 절약 > 복잡하거나 전문적인 문제가 생겼을 때 해결 능력이 떨어질 수 있음’ 
        
        따라서 ‘서비스의 목적’에 맞춰 세가지 요소가 가장 잘 만나는 지점을 찾아야 함! 
        
        </aside>
        
    - **문제 9 :** 기업이 상용 API를 사용하는 대신 오픈 소스 모델을 직접 호스팅(Self-hosting)하는 것을 선호하는 주요 이유 두 가지는 무엇입니까?
        
        <aside>
        💡
        
        **결국에는 어떤 모델이 가장 좋은지? 는 중요하지 않음, 내 결과물에 가장 적합한 모델이 무엇인지가 중요** 
        
        **그래서 모델을 직접 호스팅할지 모델 API를 사용할지에 대한 답은 활용 사례에 따라 다름, 고려해야할 상황이 총 7가지가 있는데 내가 읽어봤을 때 기업이 오픈 소스 모델을 선호하는 두가지의 이유는 아래와 같음.** 
        
        1. 데이터 프라이버시 : 기업은 매우 민감한 정보를 다룰 때가 많은데 상용 api를 쓴다는 건 데이터를 외부 서버로 전송한다는 뜻임. 엄격한 데이터 프라이버시 정책으로 조직 외부에 데이터를 전송할 수 없는 기업은 외부 api 사용 불가능
        2. 데이터 계보와 저작권: ai 관련 지적재산권법이 명확해질떄까지는 오픈소스 모델을 쓰거나 독점 모델(내부에서 직접 개발한 모델)을 선택하거나 아니면 둘다 안쓰거나 하는 중 > 기존의 저작권법은 인간의 창작물을 보호하는 것을 전제로 함. 근데 ai가 만들어 낸 것들 모두 인간의 창작물이 아니므로 그 결과물 때문에 저작권 침해 소송이 걸렸을 때 누가 책임을 져야하는지 법적으로 정해지지 않은 상태임, 법이 불확실하기 때문에 소송 위험을 최소화하는 방향으로 움직임 그래서 보수적인 선택을 하는 것. 
        </aside>
        
    - **문제 10 :** 모델 선택 시 공개 벤치마크(Public Benchmarks)를 신뢰할 수 없는 주요 이유 두 가지를 설명하십시오.
        
        <aside>
        💡
        
        1. 벤치마크 평가 시 컴퓨팅 자원이 많이 필요로 하기 때문에 평가 시 모든 벤치마크 성능을 확인하는 데 제약이 있다. 따라서 공개 벤치마크로는 파운데이션 모델의 광범위한 능력과 취약점을 모두 파악하는 데 한계가 있다.
        2. 벤치마크 선정의 어려움 - 모델마다 서로 다른 다양한 벤치마크 평가 결과가 있다. 개발자는 벤치마크를 매우 신중하게 선택하지만 현실적으로 모델마다 서로 다른 벤치마크 평가 결과가 존재한다. 사용자 입장에서는 다양한 기준으로 모델을 서로 비교대조할 수 있는지 명확하게 파악하기 어려운 문제가 있다.
        </aside>
        
    - **문제 11 :** 공개 벤치마크의 데이터 오염(Data Contamination)을 감지하는 두 가지 기술적 방법(N-gram 중복을 넘어)을 설명하십시오.
        
        <aside>
        💡
        
        1. n-gram 중복
        
        : 모델이 학습한 train set에 test set의 하나의 샘플의 13개 토큰 이상 동일한 시퀀스가 있는 경우, 데이터 리키지가 발생한 것으로 간주된다.
        
        1. Perplexity
        
        : 모델이 특정 test set 샘플에 대해 퍼플렉서티가 매우 낮다면 모델은 학습 과정에서 해당 샘플을 이미 학습했을 확률이 매우 높다고 생각할 수 있다.
        
        </aside>
        
    - **문제 12 :** 평가 파이프라인 설계의 첫 번째 단계인 시스템의 모든 구성 요소 평가에서, 턴 기반 평가(Turn-based Evaluation)와 태스크 기반 평가(Task-based Evaluation)가 서로 어떻게 다르며, 사용자 관점에서 더 중요한 평가는 무엇입니까?
        
        <aside>
        💡
        
         턴 기반 평가는 어플리케이션의 턴별 출력물의 품질을 평가하는 것이다. 예를 들면 RAG 어플리케이션을 개발하는데 1) pdf파일에서 마크다운 형식으로 표(table) 데이터를 잘 추출했는지, 2) 청킹 결과 토큰 개수에 일관성이 있는지, 3) retrieval 성능이 양호한지, etc 이와같이 세부 과정별로 나온 결과물의 품질을 평가한다. 
        
         반면에 태스크 기반 평가(작업 기반 평가)는 어플리케이션이 특정 과제를 완수했는지에 대해 평가한다(ex- 클로드가 코드 오류를 몇번의 시도만에 해결했는지). 따라서 서비스를 이용하는 사용자 관점에서는 작업 기반 평가가 더 직관적이고 중요하다고 할 수 있다.
        
        </aside>
        
    - **문제 13 :** 평가 파이프라인 설계의 평가 가이드라인 작성 단계에서, 명확한 채점 기준표(Scoring Rubrics)를 작성하고 예시를 포함하는 것이 가장 중요한 이유는 무엇입니까?
        
        <aside>
        💡
        
        </aside>
        
    - **문제 14 :** 평가 파이프라인에서 데이터 슬라이싱(Data Slicing)을 수행하여 심슨의 역설(Simpson’s paradox)을 피해야 하는 이유를 설명하십시오.
        
        <aside>
        💡
        
        - 데이터를 전체로만 묶어서 평가할 경우, 각 그룹의 샘플 크기 불균형으로 인해 심슨의 역설이 발생할 수 있음
        - 세부 그룹별로 보면 모델 B가 우수한데, 전체 평균으로는 모델 A가 더 우수한 것처럼 결과가 역전되는 현상
        - 따라서 데이터 슬라이싱을 통해 데이터를 세부 하위 그룹으로 쪼개서 평가함으로써, 특정 그룹에서의 성능 저하를 감지하고 모든 경우에 실제로 우수한 모델을 선택해야 함
        </aside>
        
    - **문제 15 :** 시스템 평가 시 A/B 테스팅이 필요한 이유를 비즈니스 목표와의 관계 측면에서 설명하십시오.
        
        <aside>
        💡
        
        - 비즈니스 목표(예: 구매율 증가)는 AI 모델의 성능뿐만 아니라 프로모션, 신제품 출시 등 외부 요인의 영향도 많이 받음
        - 단순히 지표가 올랐다고 해서 모델 성능이 좋다고 단정할 수 없음
        - A/B 테스팅은 사용자 절반에게는 기존 시스템, 나머지에는 새 모델을 노출하여 외부 변수를 동일하게 통제함. 이를 통해 비즈니스 성과가 순수하게 AI 모델 덕분인지 인과관계를 정확히 평가할 수 있음
        </aside>
        
    - **문제 16 :** OpenAI가 통계적 유의성(95% 신뢰도)을 위해 제시한 평가 샘플 크기 추정치에 대한 경험적 규칙(rule of thumb)을 설명하십시오.
        
        <aside>
        💡
        
        </aside>
        
    
    ---
    
    ## 📝 예시 답안
    
    **답안 1 :** MCQs는 검증 및 재현이 용이하며, 지식 및 추론 능력을 평가하는 데 적합하고, 무작위 기준선 대비 평가가 가능하기 때문에 선호된다.
    
    **답안 2 :** Pass@k는 모델이 생성한 k개의 코드 샘플 중 문제의 모든 테스트를 통과한 문제 비율이다. k가 커질수록 정답을 맞출 기대 확률이 높아지므로 pass@1 < pass@3 < pass@10 형태로 증가한다.
    
    **답안 3 :** LLM의 생성 성능이 향상되어 텍스트가 인간과 거의 구분되지 않기 때문에 유창성과 일관성의 중요성이 낮아졌다. 다만 약한 모델이나 저자원 언어에서는 여전히 유용하다.
    
    **답안 4 :** 지역 사실적 일관성은 출력이 **제공된 컨텍스트 내부의 근거**를 갖는지 평가한다. 글로벌 사실적 일관성은 출력이 **외부 공개 지식(Open Knowledge)**과 부합하는지 평가한다.
    
    **답안 5 :** SelfCheckGPT는 하나의 질문에 대해 여러 응답을 생성해 서로 불일치하는지를 분석하여 환각을 추정한다. SAFE는 응답을 문장 단위로 분해하여 검색으로 사실을 확인하고, AI가 문장이 검색 결과와 일치하는지 판단한다.
    
    **답안 6 :** 정치·종교적 편향은 모델이 특정 이념을 지지하는 방식으로 출력하도록 유도할 수 있다. 예: GPT-4는 더 좌파적·자유주의적, Llama는 더 권위주의적 경향이 있다는 연구 결과가 있다.
    
    **답안 7 :** IFEval은 길이·키워드·JSON 형식·불릿 개수 등 자동 검증 가능한 25가지 유형의 명령어를 모델이 정확히 따르는지 측정해 점수를 산출한다.
    
    **답안 8 :** 모델이 아무리 고품질이어도 지연 시간 또는 비용이 과도하면 실제 서비스에서 활용 가치가 떨어진다. 따라서 비용·속도 조건이 좋은 경우 낮은 품질 모델을 선택하는 사례도 있다.
    
    **답안 9 :** ① 데이터 프라이버시 보호
    
           ② API 제공업체의 약관·속도 제한에 종속되지 않고 모델을 자유롭게 수정·제어·투명
    
                하게 운영하기 위해
    
    **답안 10 :** ① 데이터 오염 가능성으로 인해 점수가 실제 능력을 반영하지 않을 수 있다.
    
             ② 벤치마크 선택 및 점수 집계 과정이 불투명한 경우가 많아 신뢰하기 어렵다.
    
    **답안 11 :**  ① N-gram 중복 분석 — 평가 샘플의 13-토큰 연속 시퀀스가 훈련 데이터에 
    
                      존재하면 오염 가능성을 의심한다.
    
             ② 퍼플렉시티 분석 — 모델이 평가 데이터에서 비정상적으로 낮은 퍼플렉시티를 
    
                 보이면 사전 노출을 의심할 수 있다.
    
    **답안 12 :** 턴 기반 평가는 메시지 단위 품질을 평가하고, 태스크 기반 평가는 전체 대화를 통해 사용자의 목표가 달성되었는지를 평가한다. 사용자 관점에서는 태스크 기반 평가가 더 중요하다.
    
    **답안 13 :**  가이드라인이 모호하면 점수도 모호해져 신뢰할 수 없는 평가가 된다. 좋은 응답의 정의를 예시와 함께 명확히 규정함으로써 평가 일관성을 확보할 수 있다.
    
    **답안 14 :** 데이터를 하위 그룹으로 나누어 성능을 평가하면 특정 입력 그룹에서의 성능 편차를 파악할 수 있고, 심슨의 역설(집계 성능과 그룹별 성능이 반대로 나타나는 현상)을 방지할 수 있다.
    
    **답안 15 :** 변경된 시스템이 비즈니스 지표 향상을 **직접 야기했는지** 확인하기 위해 A/B 테스팅이 필요하다. 외부 요인(예: 프로모션, 시즌 효과)과 분리하여 인과 관계를 측정할 수 있다.
    
    **답안 16 :** 점수 차이가 3배 줄어들수록 필요한 평가 샘플 수는 10배 증가한다는 경험적 규칙이 존재한다.
    
