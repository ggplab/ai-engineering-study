# 07단원

    - 문제
        - 1. 파인튜닝이 모델의 가중치(Weights)를 직접 조정함으로써 얻는 이점을 프롬프트 기반 방식과 비교하여 **서술하십시오.**
            
            <aside>
            💡
            
            1. 모델 행동을 구조적으로 변화시킴
            프롬프트는 지시만 바꾸지만, 파인튜닝은 가중치를 수정해 모델의 능력 자체를 재구성함, 사용자 요구 스타일·형식을 더 정확히 따르게 됨
            2. 일관된 출력 형식 준수 능력 향상
            JSON, YAML, DSL 같은 구조화된 출력 형식을 더 안정적으로 생성하도록 학습됨
            프롬프트로는 불안정할 수 있는 부분을 구조적으로 해결함
            3. 도메인 특화 성능 강화
            특정 도메인의 예시 데이터를 가중치에 반영하여 일반 모델 대비 전문적·정확한 출력을 내도록 함 (예: 특정 SQL 방언, 특정 산업 문서 스타일 등)
            4. 모델의 ‘행동적 문제’를 교정 가능
            사실은 맞지만 맥락에 안 맞는 답, 형식 미준수 등 프롬프트로는 고치기 어려운 문제를 안정적으로 해결함
            5. 예시를 내부화하여 짧은 프롬프트로도 고품질 결과 생성
            프롬프트에 예시를 반복해 넣지 않아도 됨 → 비용·지연 감소, 더 많은 예시 학습 가능함
            </aside>
            
        - 2. 사전 학습된 베이스 모델의 지식을 활용하는 전이 학습이 '샘플 효율성(Sample efficiency)'에 미치는 긍정적 영향을 **설명하십시오.**
            
            <aside>
            💡
            
            1. 이미 사전 학습된 지식을 활용하여 적은 데이터로도 빠르게 학습 가능
            베이스 모델은 방대한 데이터로 사전 학습되어 있기 때문에, 새로운 작업에서 처음부터 학습할 필요가 없음
            → 필요한 학습량(샘플 수)이 크게 감소함
            2. 새로운 작업에 필요한 능력이 이미 모델 내부에 부분적으로 존재함
            전이 학습은 기존에 학습한 일반적 언어 패턴·세계 지식·추론 능력을 재활용함
            → 파인튜닝 시에는 세부 조정만 하면 되므로 소량의 데이터로도 높은 성능을 달성함
            3. 데이터 수집·주석 비용 감소
            특정 분야(예: 법률 QA, text-to-SQL)는 데이터가 적고 주석 비용이 큰데, 전이 학습 덕분에 수백 개 수준의 데이터만으로도 충분함
            → 비용과 시간 모두 절감
            4. 모델이 새로운 작업을 더 빠르게 일반화함
            사전 학습 과정에서 이미 다양한 패턴을 경험했기 때문에, 적은 예시로도 새로운 작업의 규칙을 빠르게 파악함
            </aside>
            
        - 3. 모델의 실패 원인이 '정보 부재'인지 '행동 문제'인지에 따라 RAG와 파인튜닝 중 무엇을 선택해야 하는지 그 **기준을 제시하십시오.**
            
            <aside>
            💡
            
            1. 정보 부재(Information Missing)가 원인일 때 → RAG 선택
            모델이 필요한 사실이나 문서를 모르고 있어서 틀린 답을 하는 경우
            예: 기업 내부 매뉴얼, 최신 데이터, 전문 지식 등 모델 내부에 없는 정보를 요구할 때
            → 외부 지식 소스를 연결하는 RAG가 효과적임
            2. 행동 문제(Behavior Issue)가 원인일 때 → 파인튜닝 선택
            모델이 사실을 알고 있어도 형식을 지키지 않음, 맥락에서 벗어나 말함, 일관성이 떨어짐, 안전성 이슈 있음
            정보는 있지만 “행동 방식이 잘못된” 경우
            → 파인튜닝을 통해 가중치를 조정해 행동을 구조적으로 교정해야 함
            - 정보가 부족해서 틀린 답 → RAG
            - 행동 방식 자체가 잘못돼서 틀린 답 → 파인튜닝
            </aside>
            
        - 4. 신경망 학습의 역전파(Backpropagation) 메커니즘이 GPU 메모리 점유에 미치는 영향을 **분석하십시오.** 모델 가중치 외에 그래디언트(Gradients)와 옵티마이저 상태(Optimizer states)가 추가적인 메모리를 요구함으로써 발생하는 하드웨어 제약 사항을 **설명하십시오.**
            
            <aside>
            💡
            
            신경망 학습 과정에서는 순전파와 역전파 과정이 둘 다 이루어진다. 특히, 역전파 과정에서는 모델 가중치, 활성화 함수를 통과한 값(activations), 그래디언트, 옵티마이저 상태에 대한 정보를 저장해야 하기 때문에 추가적인 메모리를 요구하게 된다:
            
            예시) 7B-parameter model
            
            1. 16-bit format에서 모델을 로딩할 때
            
            : 2 bytes * 7B = 14GB 메모리
            
            1. Adam optimizer로 full finetuning
            
            : 7B * 3(파라미터마다 그래디언트 1개 + 옵티마이저 상태값 2개(모멘텀 및 분산 등)) * 2 bytes = 42GB 메모리
            
            1. Total memory needed = 14GB + 42GB = 56GB
            
            (activations는 포함하지 않은 메모리)
            
            </aside>
            
        - 5. BF16 포맷이 FP16과 비교하여 대규모 모델 훈련의 안정성에 기여하는 바를 **고찰하십시오.**
            
            <aside>
            💡
            
            BF16 - 8 byte range / 7 byte precision
            
            FP16 - 5 byte range / 10 byte precision
            
            BF16는 FP32와 동일한 range byte를 가지므로 동일한 수치 범위를 표현할 수 있다.
            
            반면에 FP16는 범위 비트가 적어 표현할 수 있는 최대값에 한계가 있다. 수치 범위를 벗어나면 무한대로 반올림되거나 임의의 값으로 변환될 수 있다고 한다.
            
            이에 따라 대규모 모델 훈련 중 발생하는 큰 값들이 FP16의 범위를 초과하면 모델 품질이 저하되거나 훈련이 실패할 수 있다. BF16은 FP16보다 훨씬 큰 값을 표현할 수 있어, 이러한 오버플로우 현상을 방지하고 훈련 프로세스를 안정적으로 유지할 수 있다.
            
            </aside>
            
        - 6. 양자화 인지 학습(QAT)이 저비트 모델의 추론 품질 유지에 미치는 영향을 사후 양자화(PTQ)와 비교하여 **설명하십시오.**
            
            <aside>
            💡
            
            사후 양자화(Post-Training Quantization, PTQ): 모델이 이미 완전히 훈련된 후에 양자화를 수행하는 방식
            
            한계 - 훈련이 끝난 후 정밀도를 낮추기 때문에 모델의 품질이 저하(degrade)될 수 있는 위험이 있다. 특히 8비트 이하의 매우 낮은 비트로 변환할 때 이러한 품질 유지 문제가 도전 과제가 된다.
            
            양자화 인지 학습(Quantization-Aware Training, QAT): QAT의 주된 목표는 추론 시 저비트(저정밀도) 환경에서도 높은 품질을 유지하는 모델을 만드는 것이다. 이를 위해 모델은 훈련 과정 중에 저비트(ex. 8 byte)의 동작을 시뮬레이션한다.
            
            효과 - 훈련 단계에서 양자화로 인한 오차를 미리 경험하고 이에 적응하기 때문에 최종적으로 생성된 모델은 저비트 환경에서도 고품질의 출력을 내는 법을 학습하게 된다. 이는 PTQ에서 발생할 수 있는 품질 저하 문제를 직접적으로 해결하기 위한 방법이다.
            
            </aside>
            
        - 7. 풀 파인튜닝(Full finetuning)이 대규모 모델에서 가지는 메모리 한계를 지적하고, PEFT가 어떻게 이 문제를 해결하는지 **서술하십시오.** 적은 수의 학습 파라미터로 풀 파인튜닝에 근접한 성능을 내기 위한 핵심 원리를 **기술하십시오**
            
            <aside>
            💡
            
            한계: 풀 파인튜닝은 모델의 모든 파라미터를 업데이트 하므로 막대한 메모리가 필요함
            
            PEET: PEET는 학습 가능한 파라미터의 수를 획기적으로 줄여 메모리 사용량을 최소화
            모델의 전체 가중치를 업데이트하는 대신 특정 위치에 소수의 추가 파라미터를 삽입하고 이들만 학습시켜서 메모리 요구량을 줄임
            
            핵심원리: 거대 언어 모델은 수많은 파라미터를 가지고 있지만 실제로는 매우 낮은 내제적 차원을 가짐
            사전 학습 과정이 하위 작업을 위한 압축 프레임워크 역할을 하기 때문에, 소수의 파라미터를 조정하는 것만으로 모델의 행동을 유의미하게 변화시킬 수 있음
            
            </aside>
            
        - 8. LoRA(Low-Rank Adaptation)가 원래의 가중치 행렬(*W*)을 유지하면서도 효율적으로 학습할 수 있는 수학적 원리를 **설명하십시오.**
            
            <aside>
            💡
            
            </aside>
            
        - 9. LoRA 어댑터를 베이스 모델에 병합(Merge)하여 서빙할 때와 분리하여 서빙할 때의 차이점을 **비교하여 서술하십시오.**
            
            <aside>
            💡
            
            병합방식: 학습한 조각들을 원본 모델에 아예 합쳐버리는 방식. 서비스 할 때 추가 계산이 필요 없어 속도가 빠르지만 고객별로 다른 모델을 제공해야 해서 저장 공간을 많이 차지한다.
            
            분리방식: 공유하는 원본 모델 하나는 그대로 두고 작은 학습 조각(어댑터)만 따로 보관하다가 필요할 때 끼워 쓰는 형태. 여러 작업을 하나의 서버에서 처리할 때 어댑터만 바꾸면 돼서 저장 공간을 줄이고 작업 전환이 빠르다.
            
            </aside>
            
        - 10. 4비트 NF4 양자화와 페이지드 옵티마이저(Paged optimizers)가 단일 GPU에서의 거대 모델 학습을 가능하게 만든 과정을 **설명하십시오.** 다만, 이러한 메모리 절감이 실제 학습 시간에 미칠 수 있는 부정적 영향을 함께 **기술하십시오**
            
            <aside>
            💡
            
            <NF4, paged optimizer가 단일 GPU에서 거대 모델 학습이 가능한 과정>
            
            1. NF4(normalFloat-4): QLoRA가 사용하는 4비트 형식
            - LLM은 원래 BF16을 사용, 하지만 QLoRA는 4비트로 저장햇다가(1/4로 줄어듬) 순전파와 역전파를 계산할 때만 다시 BF16으로 역양자화해서 씀
            - 모델의 데이터(가중치)는 보통 정규분포를 그리는데 NF4는 이 분포에 딱 맞춰 데이터를 압축하는 방식, 그래서 예전에는 GPU 4대가 필요했던 모델이라면 이젠 단일 GPU 안에 데이터를 넣을 수 있게 된 것임.
            1. Paged optimizer: GPU 메모리가 꽉 차기 전에, 안쓰는 메모리를 CPU로 옮겨두고 필요하면 다시 가져오는 방식, 메모리가 부족해서 학습이 멈추는 일이 없기 떄문에 단일 GPU에서도 긴 문장을 학습할 수 있게 됨
            
            <메모리 절감이 실제 학습 시간에 미칠 수 있는 부정적 영향>
            
            : 메모리를 아낀 대신 시간을 손해 보게 됨 
            
            1. NF4: 메모리 사용량을 줄여줄 수 있지만, 양자화하고 다시 되돌리는 과정에서 시간이 더 걸려서 학습시간이 늘어날 수 있음.
            2. Paged optimizer: CPU 와 GPU 사이의 길은 GPU 내부 통로보다 훨씬 좁고 느려서 데이터 이동 때문에 전체 학습 속도가 늘어남.
            
            </aside>
            
        - 11. 여러 파인튜닝된 모델을 하나로 합치는 '모델 머징'이 다중 작업(Multi-task) 수행에 미치는 영향을 **서술하십시오.** 별도의 모델 앙상블 방식과 비교하여 추론 비용 측면에서 가지는 장점을 **설명하십시오**
            
            <aside>
            💡
            
            - 모델 머징: 서로 다른 작업에 대해 각각 파인튜닝을 병렬로 진행한 다음, 완료 되면 서로 다른 모델들의 가중치만 골라서 합치는 방식
            - 앙상블: 각 구성 모델을 그대로 놔둔, 모델 출력만 결합하는 방식
            
            <multi-task 수행에 미치는 영향>
            
            : 파인 튜닝을 한 모델의 지식만 골라서 합치는 방식이기 때문에 하나의 모델이 A 작업도 잘하고 B 작업도 잘하는 다중 작업 수행에 유리하게 됨. 
            
            <앙상블 방식과 비교해 추론 비용 측면에서의 장점>
            
            - 앙상블은 성능을 끌어올릴 순 있지만, 요청 하나당 여러번 추론을 돌려야해서 비용이 더 든다. (모델을 3번 돌려야 하니까 GPU 메모리도 시간도 그만큼 더 든다)
            - 하지만, 모델 머징의 경우에는 가중치 자체가 합쳐진 거라, 최종 결과물은 결국 모델 1개의 용량임. 그래서 추가적인 하드웨어 비용, 지연시간 없이 여러 모델의 성능을 동시에 누릴 수 있음.
            </aside>
            
        - 12. 순차적 파인튜닝(Sequential finetuning)에서 발생하는 치명적 망각 현상을 정의하고, 모델 머징이 이를 어떻게 완화할 수 있는지 **방안을 제시하십시오.**
            
            <aside>
            💡
            
            치명적 망각이란, 모델이 새로운 작업으로 순차적으로 파인튜닝될 때 이전에 학습한 내용이 **급격히 사라지는 현상**
            
            1. **응답의 정확성 및 전반적인 품질 향상:** 모델은 제공된 관련 정보를 활용하여 더 **정확하고 상세하며 유용한 응답**을 생성할 수 있습니다. 컨텍스트는 모델이 각 쿼리에 맞춰 적절한 답변을 구성하도록 돕는 핵심 요소입니다.
            
            2. **환각(Hallucination) 현상 방지:** 모델에게 필요한 정보가 주어지지 않으면 신뢰할 수 없는 **내부 지식에 의존**하게 되어 거짓 정보를 지어낼 위험이 큽니다. 충분한 컨텍스트를 제공하면 이러한 **환각을 완화하고 응답의 사실적 일관성과 신뢰성**을 높일 수 있습니다.
            
            **비유하자면,** 컨텍스트를 제공하는 것은 학생에게 시험을 볼 때 기억력에만 의존하게 하는 대신, 옆에 **관련 참고 서적**을 두고 정확한 답을 찾게 도와주는 것과 같습니다
            
            </aside>
            
        - 13. 모델 가중치를 단순 평균(Weight Averaging)하는 것보다 SLERP 방식을 사용하는 것이 유리한 이유를 기하학적 관점에서 **설명하십시오.**
            
            <aside>
            💡
            
            </aside>
            
        - 14. 학습률의 크기가 손실 곡선(Loss curve)의 변동성에 미치는 영향을 분석하고, 적절한 학습률 범위를 찾는 과정을 **설명하십시오.**
            
            <aside>
            💡
            
            **1. 학습률 크기가 손실 곡선의 변동성에 미치는 영향**
            
            - **학습률이 너무 큰 경우:** 모델 파라미터가 최적의 지점을 지나쳐 버리기 때문에 손실 곡선이 심하게 요동(fluctuation)치며 불안정한 모습을 보입니다.
            - **학습률이 너무 작은 경우:** 손실 곡선은 매우 안정적이지만 **감소 속도가 지나치게 느려** 목표 지점에 도달하는 데 너무 오랜 시간이 걸리게 됩니다.
            
            **2. 적절한 학습률 범위를 찾는 과정**
            
            - **초기 범위 설정:** 보편적인 최적값은 없으므로 보통 **1e-7에서 1e-3 사이**의 범위에서 실험을 시작합니다.
            - **사전 학습 지표 활용:** 사전 학습(Pre-training)이 끝난 시점의 학습률에 **0.1에서 1 사이의 상수를 곱한 값**을 시작점으로 설정하는 것이 일반적인 관행입니다.
            - **곡선 모니터링 및 조정:** **손실 곡선이 안정적으로 유지되는 한 가장 높은 학습률**을 선택하는 것이 학습 효율 면에서 가장 좋습니다.
            - **스케줄링 도입:** 학습 과정 전체에서 동일한 값을 쓰는 대신, 진행 상황에 따라 학습률을 점진적으로 조정하는 학습률 스케줄(Learning rate schedules)을 적용하여 성능을 최적화합니다
            </aside>
            
        - 15. 배치 크기 설정이 학습 안정성에 미치는 영향과 GPU 메모리 한계를 극복하기 위한 '그래디언트 누적(Gradient accumulation)' 기술의 역할을 **설명하십시오.**
            
            <aside>
            💡
            
            </aside>
            
            - 답안
                1. 파인튜닝은 모델의 **가중치(Weights)를 직접 조정**하여 특정 작업에 적응시키는 과정인 반면, 프롬프트 방식은 가중치 수정 없이 지시사항과 문맥만을 제공합니다. 파인튜닝은 특히 **모델의 지시 이행 능력(Instruction-following)**과 특정 출력 형식(JSON, YAML 등) 준수 능력을 개선하는 데 효과적입니다. 다만, 프롬프트 방식에 비해 데이터 확보, 하드웨어 자원, 머신러닝 전문 인력 등 **더 높은 초기 투자 비용**이 발생한다는 트레이드오프가 존재합니다
                2. 전이 학습은 한 작업에서 얻은 지식을 새로운 관련 작업 학습을 가속화하는 데 활용하는 기술입니다. 이는 **샘플 효율성(Sample efficiency)**을 크게 높여주는데, 예를 들어 법률 질의응답 모델을 처음부터 학습시키려면 수백만 개의 예시가 필요하지만, 잘 훈련된 베이스 모델을 파인튜닝하면 **단 몇 백 개의 예시만으로도** 효과적인 학습이 가능합니다
                3. 실패 원인이 **'정보 부재(Information-based)'**라면 외부 지식에 접근할 수 있는 **RAG**가 적합하며, **'행동 문제(Behavior-based)'**라면 출력 스타일과 형식을 교정하는 **파인튜닝**이 적합합니다. "파인튜닝은 형식(Form)을 위한 것이고, RAG는 사실(Facts)을 위한 것이다"라는 원칙에 따라, 최신성 유지와 사실적 정확성이 중요하다면 RAG를, 복잡한 구문 준수와 스타일 최적화가 중요하다면 파인튜닝을 선택해야 합니다
                4. 학습 시에는 순전파뿐만 아니라 **역전파(Backpropagation)** 과정을 수행해야 하므로 추론보다 훨씬 많은 메모리가 필요합니다. 메모리 점유의 주요 원인은 모델 가중치 외에도 **그래디언트(Gradients)**와 **옵티마이저 상태(Optimizer states)**입니다. 특히 Adam 옵티마이저는 학습 가능한 파라미터당 2개의 추가 상태 값을 저장하므로 메모리 요구량을 크게 증가시킵니다
                5. BF16과 FP16은 모두 16비트를 사용하지만 비트 배분이 다릅니다. FP16은 정밀도(10비트)에 집중하여 수치 범위가 좁은 반면, **BF16은 범위(8비트)**에 더 많은 비트를 할당하여 FP32와 동일한 수치 범위를 가집니다. 따라서 BF16은 대규모 모델 학습 시 발생하는 **큰 수치(Out-of-bound)를 더 안정적으로 처리**할 수 있지만, FP16에 비해 정밀도는 다소 낮다는 특징이 있습니다
                6. **PTQ**는 학습이 완료된 후 모델을 저비트로 변환하는 방식으로 구현이 간단하여 개발자들에게 흔히 사용됩니다. 반면, **QAT**는 학습 과정에서 저비트 환경을 시뮬레이션하여 **추론 시 품질 저하를 최소화**합니다. QAT는 최종 모델의 정확도는 높지만, 학습 계산은 여전히 고정밀도로 수행되어 학습 시간이 단축되지 않거나 오히려 늘어날 수 있다는 트레이드오프가 있습니다
                7. 최근의 거대 모델은 모든 파라미터를 업데이트하는 '풀 파인튜닝' 시 단일 GPU 메모리를 초과하는 막대한 자원을 요구합니다. **PEFT**는 전체 가중치 중 극히 일부(수 퍼센트 미만)만 업데이트하거나 추가 모듈을 학습시킴으로써 **메모리 발자국을 획기적으로 줄여줍니다**. 이는 성능 면에서 풀 파인튜닝에 근접하면서도 훨씬 적은 자원으로 학습을 가능케 합니다
                8. LoRA는 원래의 고차원 가중치 행렬 *W*를 고정한 채, 이를 두 개의 **저차원 행렬** *A***와** *B***의 곱**으로 분해하여 이 작은 행렬들만 학습시킵니다. **랭크(*r*)**가 작을수록 학습할 파라미터 수와 메모리 사용량은 줄어들지만, 모델의 표현력이 제한될 수 있으며, 반대로 랭크가 너무 높으면 오버피팅의 위험이 생길 수 있습니다
                9.  LoRA 어댑터를 베이스 모델에 **병합(Merge)**하면 추론 지연 시간이 없으나, 여러 어댑터를 서빙할 때 각각 전체 모델을 저장해야 하므로 저장 공간 낭비가 큽니다. 반면, 어댑터를 **분리**하여 유지하면 하나의 베이스 모델만 저장하고 100개의 작은 어댑터만 관리하면 되므로 **다중 고객(Multi-tenant) 서빙 효율성**이 비약적으로 향상되지만, 추론 시 약간의 지연 시간이 발생할 수 있습니다
                10. QLoRA는 모델 가중치를 **4비트 NF4 포맷**으로 양자화하고, 메모리 부족 시 CPU로 데이터를 전송하는 **페이지드 옵티마이저(Paged optimizers)**를 사용하여 65B 모델을 단일 48GB GPU에서 학습할 수 있게 했습니다. 그러나 양자화 및 역양자화 단계가 추가되어 **학습 시간이 늘어날 수 있다는 제약**이 따릅니다
                11. 모델 머징은 여러 파인튜닝된 모델을 하나로 합쳐 **다양한 능력을 갖춘 단일 모델**을 만드는 기술입니다. 이는 모델 출력값만을 조합하는 앙상블 방식과 달리, 단 한 번의 추론 호출만 필요하므로 **추론 비용을 획기적으로 절감**하면서도 여러 작업의 성능을 동시에 확보할 수 있습니다
                12. 순차적으로 여러 작업을 학습시키면 이전 지식을 잃어버리는 '치명적 망각'이 발생합니다. 모델 머징을 이용하면 각 작업을 **병렬로 독립적으로 학습시킨 후 합치기 때문에** 이러한 망각 현상을 피할 수 있습니다. 이는 개별 작업의 전문성을 유지하면서도 모델의 범용적 성능을 보존하는 데 유리합니다
                13. 가중치를 단순히 산술 평균하는 방식과 달리, **SLERP**는 가중치 벡터를 구체 상의 점으로 간주하고 **최단 경로(Shortest path)를 따라 보간**합니다. 이는 단순 평균이 가중치의 기하학적 특성을 훼손할 수 있는 문제를 보완하여, 모델 고유의 특성을 더 잘 보존하면서 자연스러운 결합을 가능하게 합니다
                14. 학습률은 매 학습 단계에서 가중치를 얼마나 변경할지 결정하는 '보폭'입니다. 학습률이 너무 크면 **손실 곡선이 심하게 요동치며 발산**할 수 있고, 너무 작으면 학습 속도가 지나치게 느려집니다. 따라서 초기에는 큰 보폭으로 빠르게 수렴시키고 끝으로 갈수록 보폭을 줄이는 **스케줄링**을 통해 최적해에 정밀하게 도달해야 합니다
                15. 배치 크기가 클수록 여러 예시의 신호를 종합하여 **안정적인 업데이트**가 가능하지만, 더 많은 GPU 메모리가 소요됩니다. 메모리 제약으로 큰 배치를 쓰지 못할 때, 여러 배치의 그래디언트를 모았다가 한 번에 업데이트하는 **그래디언트 누적(Gradient accumulation)** 기술을 통해 하드웨어 한계를 극복하고 안정적인 학습을 수행할 수 있습니다
                
